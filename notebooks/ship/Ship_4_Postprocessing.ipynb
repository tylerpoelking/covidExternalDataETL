{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "spark.version\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# UPDATE weekly with weekending date of complete shipments data\n",
    "latest_curr_date = '2020-05-02' # Since current week is incomplete and future weeks of shipments in actuals data are outliers! \n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Import weeknumber to weekending mapping file. This includes only 2020 weeks till Aug 8th (scope of project). STATIC file, no need to update.\n",
    "dates = spark.read.format('parquet').load(\"/user/bwn2456/CBDA/week_to_we_map.parquet\")\n",
    "# Read in raw shipments data\n",
    "cbda_ship = spark.table(\"default.cbda_ship\")\n",
    "# For null weekending in cbda_ship (records which do not have corresponding sales in 2020), use weeknumber to weekending mapping file to pull in weekending\n",
    "# Inner join with date mapping file since we are only considering model forecasts till Aug 1st week\n",
    "cbda_ship = cbda_ship.join(dates.withColumnRenamed(\"date\",\"date_ref\"), how='inner', on='weekOfYear').withColumn(\"date\", coalesce(col(\"date\"),col(\"date_ref\")))\n",
    "# Read in shipments model forecasts\n",
    "forecast_df = spark.read.format('parquet').option('header','true').load('/user/bwn2456/CBDA/ship_model_results.parquet.gzip')\n",
    "forecast_df = forecast_df.withColumn(\"week_ending_date\", next_day(\"week_ending_date\", \"saturday\"))\n",
    "print(forecast_df.count())\n",
    "#z.show(forecast_df)\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "from pyspark.sql.types import *\n",
    "channel_mapping = spark.createDataFrame(\n",
    "[\n",
    "(\"01\", \"Distribution Channel 01\"),\n",
    "(\"10\", \"Other\"),\n",
    "(\"11\", \"DSD Bis Intercompany\"),\n",
    "(\"12\", \"DSD Pizza Intercomp\"),\n",
    "(\"20\", \"\"\"Warehouse/Exports\"\"\"),\n",
    "(\"30\", \"Foodservice\"),\n",
    "(\"40\", \"DSD Pizza\"),\n",
    "(\"45\", \"DSD\"),\n",
    "(\"50\", \"KFI\"),\n",
    "(\"55\", \"Plant Ingredient\"),\n",
    "(\"60\", \"Imports\"),\n",
    "(\"65\", \"Bulk FS - Specialty\"),\n",
    "],\n",
    "StructType([StructField('bic_zdistr_ch',StringType(), True), StructField('channel_desc',StringType(), True)]) # add your columns label here\n",
    ")\n",
    "#z.show(channel_mapping)\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Retailer ID to retailer description mapping\n",
    "agg_raw = spark.table(\"default.raw_ship_agg\")\n",
    "retailer_map = agg_raw.select(\"ac_scbm_id\",\"ac_scbm_desc\").distinct()\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Rename columns and join in channel mapping\n",
    "print(forecast_df.count())\n",
    "forecast_df_cols = forecast_df.withColumnRenamed(\"week_ending_date\",\"date\").withColumnRenamed(\"retailer\",\"ac_scbm_id\").withColumnRenamed(\"retailer_desc\",\"ac_scbm_desc\").withColumnRenamed(\"mdlz_business\",\"management_grouping_desc\").withColumnRenamed(\"mdlz_category\",\"product_category_name\").withColumnRenamed(\"mdlz_brand\",\"product_family_desc\").withColumnRenamed(\"mdlz_ppg\",\"promoted_product_group\").withColumnRenamed(\"pos_qty\",\"sales_pounds\").withColumnRenamed(\"pos_dollar\",\"gross_sales\").withColumnRenamed(\"week_of_year\",\"weekOfYear\").withColumnRenamed(\"channel\",\"bic_zdistr_ch\")\n",
    "print(forecast_df_cols.count())\n",
    "forecast_joined = forecast_df_cols.join(channel_mapping, how='left',on=['bic_zdistr_ch'])\n",
    "print(forecast_joined.count())\n",
    "# z.show(forecast_joined)\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Convert to date\n",
    "forecast_joined = forecast_joined.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Pull only records in forecast df that do not overlap with cbda_ship (current / past df)\n",
    "forecast_no_past = forecast_joined.filter(col(\"date\") >= latest_curr_date).select(\"ac_scbm_id\", \"ac_scbm_desc\", \"state\", \"management_grouping_desc\", \"product_category_name\", \"product_segment_name\", \"product_family_desc\", \"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"date\", \"forecast1\", \"forecast_quantity\", \"pos_qty_ly\", \"pos_dollar_ly\",\"promo_uplift\", \"channel_desc\").withColumnRenamed(\"forecast1\",\"forecast_percent\").withColumnRenamed(\"pos_qty_ly\",\"pos_qty_ly_filled\").withColumnRenamed(\"pos_dollar_ly\",\"pos_dollar_ly_filled\")\n",
    "# Decide on the filter - did not have any effect previously - first value printed should be 0 !\n",
    "#print(forecast_no_past.filter(\"state IS NULL OR ac_scbm_id IS NULL\").head())\n",
    "print(forecast_no_past.filter(\"state IS NULL OR ac_scbm_id IS NULL\").count()) # should be 0\n",
    "forecast_no_past = forecast_no_past.filter(\"state IS NOT NULL AND ac_scbm_id IS NOT NULL\")\n",
    "# Number of remaining records is ~1.3M on 5/02 data\n",
    "print(forecast_no_past.count())\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Bring in forecast columns (qty & %) into main df AND pull in values for the latest date for continuous visualization purposes\n",
    "cbda_ship_w_fore_cols = cbda_ship.filter(col(\"date\") <= latest_curr_date).join(forecast_no_past.filter(col(\"date\") == latest_curr_date), how=\"left\", on=[\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_segment_name\",\"product_family_desc\", \"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\",\"date\"] )\n",
    "# Include promo_uplift column\n",
    "cbda_ship_w_fore_cols = cbda_ship_w_fore_cols.withColumn(\"promo_uplift\", lit(0))\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Prepare table for union by brining in addl columns\n",
    "forecast_w_cbdaship_cols = forecast_no_past.filter(col(\"date\") > latest_curr_date).join(cbda_ship.filter(col(\"date\") < latest_curr_date), how=\"left\", on=[\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_segment_name\",\"product_family_desc\", \"promoted_product_group_desc\",\"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\",\"date\"] ).filter(col(\"date\") != latest_curr_date).select(*cbda_ship_w_fore_cols.columns)\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "cbda_ship_w_forecasts = cbda_ship_w_fore_cols.union(forecast_w_cbdaship_cols).sort([\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_family_desc\", \"product_segment_name\",\"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\",\"date\"])\n",
    "# Pull in LY numbers from 2019 corresponding to future weeks (i.e. April 4, 2019 and beyond till August 2019)\n",
    "cbda_ship_w_forecasts = cbda_ship_w_forecasts.withColumn(\"stly_sales_pounds\", coalesce(col(\"stly_sales_pounds\"),col(\"pos_qty_ly_filled\")).cast('double'))\n",
    "cbda_ship_w_forecasts = cbda_ship_w_forecasts.withColumn(\"stly_gross_sales\", coalesce(col(\"stly_gross_sales\"),col(\"pos_dollar_ly_filled\")).cast('double'))\n",
    "cbda_ship_w_forecasts = cbda_ship_w_forecasts.drop(\"pos_qty_ly_filled\",\"pos_dollar_ly_filled\")\n",
    "# For Tableau visualization - pull in actuals for forecasts field for the latest actual week\n",
    "cbda_ship_w_forecasts = cbda_ship_w_forecasts.withColumn(\"forecast_quantity\", when(col(\"date\") == latest_curr_date, col(\"sales_pounds\")).otherwise(col(\"forecast_quantity\")))\n",
    "# For Tableau visualization - Fill NAs with 0 for promo_uplift\n",
    "cbda_ship_w_forecasts = cbda_ship_w_forecasts.fillna(0, subset=['promo_uplift'])\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Pull in correct last year numbers\n",
    "cbda_ship_ly = cbda_ship.select(\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_family_desc\", \"product_segment_name\", \"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\",\"date\",\"stly_sales_pounds\",\"stly_gross_sales\")\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts.drop(\"stly_sales_pounds\",\"stly_gross_sales\").join(cbda_ship_ly, how=\"outer\", on=[\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_family_desc\", \"product_segment_name\", \"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\",\"date\"])\n",
    "# For Tableau visualization - Fill NAs with 0 for promo_uplift\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts_and2019Only.fillna(0, subset=['promo_uplift'])\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "# Correct MCLANE and DOT TOTAL\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts_and2019Only.withColumn(\"forecast_quantity\", when(expr(\"(state = 'IL' or state = 'AR') AND (ac_scbm_desc = 'MCLANE' or ac_scbm_desc = 'DOT TOTAL')\") & (col(\"date\") > lit(latest_curr_date)), col(\"stly_sales_pounds\")).otherwise(col(\"forecast_quantity\")))\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts_and2019Only.withColumn(\"forecast_quantity\", when((col(\"bic_zdistr_ch\") == '45') & (col(\"ac_scbm_desc\") == \"WALMART SC/DISCOUNT\") & (col(\"promoted_product_group_desc\") == 'FAMILY SIZE OREO PPG') & (col(\"date\") > latest_curr_date), expr(\"forecast_quantity - ((forecast_quantity - stly_sales_pounds)/2) \")).otherwise(col(\"forecast_quantity\")))\n",
    "# Correct ECOMM\n",
    "windowval = (Window.partitionBy(\"ac_scbm_id\", \"ac_scbm_desc\",\"state\", \"management_grouping_desc\", \"product_category_name\", \"product_family_desc\", \"product_segment_name\", \"promoted_product_group_desc\", \"promoted_product_group\", \"bic_zdistr_ch\", \"channel_desc\").orderBy('date')\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "cbda_ship_w_forecasts_w_cumsum = cbda_ship_w_forecasts_and2019Only.withColumn('cum_sum', F.sum('forecast_quantity').over(windowval))\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts_w_cumsum.withColumn('forecast_quantity', when( expr(\"ac_scbm_desc = 'ECOMM DIRECT' AND channel_desc = 'DSD'\") , col(\"cum_sum\")).otherwise(col(\"forecast_quantity\"))).drop(\"cum_sum\")\n",
    "# Temp Fix for infinity value\n",
    "cbda_ship_w_forecasts_and2019Only = cbda_ship_w_forecasts_and2019Only.filter(\"NOT(ac_scbm_id = 'US3000050' AND state = 'GA' AND promoted_product_group = 'LM9' AND bic_zdistr_ch = '45')\")\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "print(cbda_ship_w_forecasts_and2019Only.count())\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Write out for Tableau\n",
    "cbda_ship_w_forecasts_and2019Only.createOrReplaceTempView(\"cbda_ship_w_forecasts\")\n",
    "spark.sql(\"drop table if exists default.cbda_ship_w_forecasts\")\n",
    "spark.sql(\"create table default.cbda_ship_w_forecasts as select * from cbda_ship_w_forecasts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
