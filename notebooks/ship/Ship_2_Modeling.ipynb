{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "import pyodbc\n",
    "import pysftp\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import requests, zipfile, io\n",
    "from datetime import datetime, timedelta\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BOX_PATH = f'/Users/rohitkg/Box Sync/Mondelez: Demand forecasts during COVID-19/4. EDA & Descriptive analytics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize key date variables to be used throughout the script\n",
    "max_train_date = '2020-05-09'\n",
    "max_forecast_date = '2020-08-08'\n",
    "growth_peak_dates = ['2020-03-14','2020-03-21'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE with your Mondelez LAN ID & Password to allow the script to connect to Hive and download the raw POS dataset\n",
    "mdlz_lan_id = \"bwn2456\"\n",
    "mdlz_lan_pwd = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Hive to read in POS raw data\n",
    "CONNECTION_STRING = ';'.join(f\"\"\"\n",
    "Description=Hortonworks Knox DSN\n",
    "Driver=/opt/cloudera/hiveodbc/lib/universal/libclouderahiveodbc.dylib\n",
    "Host=mdzusvpclhdp101.mdzprod.local\n",
    "port=8443\n",
    "HttpPath=gateway/mondelez/hive\n",
    "schema=default\n",
    "ServiceDiscoveryMode=0\n",
    "HiveServerType=2\n",
    "AuthMech=3\n",
    "ThriftTransport=2\n",
    "SSL=1\n",
    "TwoWaySSL=0\n",
    "AllowSelfSignedServerCert=1\n",
    "uid={mdlz_lan_id}\n",
    "pwd={mdlz_lan_pwd}\n",
    "\"\"\".splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to edge node to write out POS model output data\n",
    "edge_node = pysftp.Connection(host=\"10.54.252.11\", username=mdlz_lan_id, password=mdlz_lan_pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [0. Refresh Steps](#first-bullet)\n",
    "* [1. Read in Shipments Data](#second-bullet)\n",
    "* [2. Read in State Projections](#third-bullet)\n",
    "* [3. Pre-Processing of Modeling Data](#fourth-bullet)\n",
    "* [4. Naive Modeling](#fifth-bullet)\n",
    "* [5. Milestone Modeling + Prophet Model](#sixth-bullet)\n",
    "* [6. Final Output](#seventh-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Refresh Steps <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Ship Input dataset\n",
    "with pyodbc.connect(CONNECTION_STRING, autocommit=True) as conn:\n",
    "    ship_raw = pd.read_sql(\"SELECT * FROM default.cbda_ship_model_input\", conn)\n",
    "# Drop prefix created when reading the data from Hive table\n",
    "ship = ship_raw.copy()\n",
    "ship.columns = [i.split(\".\")[1] for i in ship.columns.values]\n",
    "    \n",
    "# Refresh State Projections - Point it to Box!\n",
    "projections_dates = pd.read_csv(f'{BOX_PATH}/Data/Projections/peak_deaths.csv')\n",
    "projections = pd.read_csv(f'{BOX_PATH}/Data/Projections/projections_state.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in Shipments Data <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters for shipments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship['week_ending_date'] = pd.to_datetime(ship['week_ending_date'])\n",
    "\n",
    "# Create a unique identifier for PPG \n",
    "ppg_id_cols = ['promoted_product_group_desc', 'product_segment_name', 'retailer_desc','channel','mdlz_business', 'mdlz_category', 'mdlz_brand','mdlz_ppg']\n",
    "ship['ppg_id'] = ship[ppg_id_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# Create a unique identifier for PPG/State/Retailer \n",
    "cols = ['ppg_id', 'state', 'retailer']\n",
    "ship['sell_id'] = ship[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "\n",
    "ship_main = ship[ship['week_ending_date']>'2019-01-01'] # Week Ending Filter from January 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSINESS FILTERS PROVIDED BY MDLZ \n",
    "og_len = len(ship_main)\n",
    "\n",
    "#NEW/SHIPMENTS SPECIFIC - Only data before today\n",
    "ship_main = ship_main[ship_main['week_ending_date']<=max_train_date]\n",
    "\n",
    "print(f'rows lost including only data before today {100*(og_len - len(ship_main))/og_len} %')\n",
    "og_len = len(ship_main)\n",
    "ship_main = ship_main[~ship_main['mdlz_category'].isin(['None','Cookie','Display PRD'])] #Excluding low value categories \n",
    "print(f'rows lost Excluding low value categories {100*(og_len - len(ship_main))/og_len}%')\n",
    "og_len = len(ship_main)\n",
    "ship_main = ship_main [~((ship_main['mdlz_ppg']=='') | (ship_main['mdlz_ppg'].isnull()))] # Excluding blank and null PPG values\n",
    "print(f'rows lost Excluding blank and null PPG values {100*(og_len - len(ship_main))/og_len}%')\n",
    "og_len = len(ship_main)\n",
    "ship_main = ship_main[~((ship_main['mdlz_business']=='') & (ship_main['mdlz_category']=='') & (ship_main['mdlz_brand']=='') & (ship_main['mdlz_ppg']!=''))] # Excluding PPGs with blank product hierarchy\n",
    "print(f'rows lost Excluding PPGs with blank product hierarchy {100*(og_len - len(ship_main))/og_len}%')\n",
    "og_len = len(ship_main)\n",
    "ship_main = ship_main[(ship_main['pos_dollar']>=0.0) & (ship_main['pos_qty']>=0.0)] # Remove returns\n",
    "print(f'rows lost Remove returns {100*(og_len - len(ship_main))/og_len}%')\n",
    "og_len = len(ship_main)\n",
    "ship_main = ship_main[~(((ship_main['pos_dollar'].isna()) & (ship_main['pos_qty'].isna())))] # Remove null sales\n",
    "print(f'rows lost Remove null sales {100*(og_len - len(ship_main))/og_len}%')\n",
    "og_len = len(ship_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(ship_main['pos_qty'].isna().sum()==0)\n",
    "assert(ship_main['pos_dollar'].isna().sum()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "print (\"ship Summary - Before the filters are applied\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(ship)))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(ship['pos_dollar'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(ship['pos_qty'])))\n",
    "print (\"Number of unique products\",ship['sell_id'].nunique())\n",
    "\n",
    "print (\"\\nship Summary - After the filters are applied\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(ship_main)))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(ship_main['pos_dollar'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(ship_main['pos_qty'])))\n",
    "print (\"Number of unique products:\",ship_main['sell_id'].nunique())\n",
    "\n",
    "print (\"\\n Revenue contribution (%) lost due to the filters\",round((1 - (np.nansum(ship_main['pos_dollar'])/np.nansum(ship['pos_dollar'])))*100,2))\n",
    "print (\"\\n Volume contribution (%) lost due to the filters\",round((1 - (np.nansum(ship_main['pos_qty'])/np.nansum(ship['pos_qty'])))*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the YoY Growth by State, Retailer, PPG (Consider only those weeks which have both 2020 and 2019 sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_main['week_of_year'] = ship_main['week_ending_date'].dt.week\n",
    "ship_main['year'] = ship_main['week_ending_date'].dt.year\n",
    "ship_main_2020 = ship_main[ship_main['year']==2020]\n",
    "ship_main_2019 = ship_main[ship_main['year']==2019]\n",
    "\n",
    "only_2019_sell_ids = set(ship_main_2019['sell_id'])-set(ship_main_2020['sell_id'])\n",
    "\n",
    "ship_main_2019 = ship_main_2019.drop('week_ending_date', axis = 1)\n",
    "ship_main_2019 = ship_main_2019.drop(['year','sell_id','ppg_id'], axis = 1)\n",
    "ship_main_2020 = ship_main_2020.drop('year', axis = 1)\n",
    "\n",
    "ship_main_new = ship_main_2020.merge(ship_main_2019, on = ['state', 'promoted_product_group_desc', \n",
    "                                                           'product_segment_name', 'retailer_desc','channel',\n",
    "                                                           'retailer', 'mdlz_business', 'mdlz_category', \n",
    "                                                           'mdlz_brand', 'mdlz_ppg', 'week_of_year'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_main_new = ship_main_new.rename(columns={'pos_qty_x':'pos_qty_ty', 'pos_dollar_x':'pos_dollar_ty', 'pos_qty_y':'pos_qty_ly', 'pos_dollar_y':'pos_dollar_ly'})\n",
    "ship_main_new['Growth_perc_sales'] = (ship_main_new['pos_dollar_ty'] - ship_main_new['pos_dollar_ly']) / ship_main_new['pos_dollar_ly']\n",
    "ship_main_new['Growth_perc_qty'] = (ship_main_new['pos_qty_ty'] - ship_main_new['pos_qty_ly']) / ship_main_new['pos_qty_ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********\n",
    "print (\"Number of unique products before the YoY join\",ship_main['sell_id'].nunique())\n",
    "print (\"Number of unique products after the YoY join\",ship_main_new['sell_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read in State Projections <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_week_ending(date):\n",
    "    start = date - timedelta(days=date.weekday())\n",
    "    end = start + timedelta(days=5)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_week_ending_incsun(peak_date):\n",
    "    dt = datetime.strptime(peak_date, '%Y-%m-%d')\n",
    "    \n",
    "    if dt.weekday()!=6:  \n",
    "        start = dt - timedelta(days=dt.weekday())\n",
    "        end = start + timedelta(days=5)\n",
    "    \n",
    "    if dt.weekday()==6:\n",
    "        start = dt - timedelta(days=dt.weekday())\n",
    "        end = start + timedelta(days=12)\n",
    "    \n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections['date'] = pd.to_datetime(projections['date'])\n",
    "projections['week_of_year'] = projections['date'].dt.week\n",
    "projections['week_ending_date'] = projections['date'].apply(lambda x: give_week_ending(x))\n",
    "\n",
    "projections_weekly = projections.groupby(['State','week_ending_date','week_of_year']).agg({'deaths_mean':'sum','new_pop_affected':'sum'}).reset_index()\n",
    "projections_weekly.rename(columns={'State':'state'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections_dates['peak_deaths_week'] = projections_dates['peak_deaths_date'].apply(lambda x: give_week_ending_incsun(x))\n",
    "projections_dates['thirtyperc_deaths_week'] = projections_dates['end_day'].apply(lambda x: give_week_ending_incsun(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-Processing of Modeling Data <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of Products based on it's 2019 and 2020 Lifespan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Filtering of Products based on Lifespan - Milestone modeling of data requires the products to have complete lifespan for 2019 and 2020 POS weeks. Hence the modeling data will be divided into datasets - \n",
    "1. Complete History - Milestone Modeling \n",
    "2. Insufficient History - Naive Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reattach the 2019 dataset \n",
    "ship_main_2019 = ship_main[ship_main['year']==2019]\n",
    "\n",
    "# Create a unique identifier for PPG \n",
    "ship_main_2019['ppg_id'] = ship_main_2019[ppg_id_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# Create a unique identifier for PPG/State/Retailer \n",
    "cols = ['ppg_id', 'state', 'retailer']\n",
    "ship_main_2019['sell_id'] = ship_main_2019[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many product groups have max weeks in 2020? \n",
    "num_2020_weeks = ship_main_new['week_ending_date'].nunique()\n",
    "unique_product_group = ship_main_new.groupby(['sell_id']).agg({'week_ending_date': lambda x: x.nunique()}).reset_index()\n",
    "print (f\"losing number of products with lifespan less than {num_2020_weeks} weeks of data (%)\",round(1 - (len(unique_product_group[unique_product_group['week_ending_date']==num_2020_weeks]) / len(unique_product_group)),2)*100)\n",
    "\n",
    "complete_2020 = unique_product_group[unique_product_group['week_ending_date']==num_2020_weeks] \n",
    "incomplete_2020 = unique_product_group[unique_product_group['week_ending_date']<num_2020_weeks] \n",
    "\n",
    "print (f\"Losing 2020 revenue contribution (%) due to dropping products with less than {num_2020_weeks} weeks of data\",round(1-sum(ship_main_new.merge(complete_2020,on=['sell_id'],how='inner')['pos_dollar_ty']) / sum(ship_main_new['pos_dollar_ty']),2)*100)\n",
    "print (f\"Losing 2020 volume contribution (%) due to dropping products with less than {num_2020_weeks} weeks of data\",round(1-sum(ship_main_new.merge(complete_2020,on=['sell_id'],how='inner')['pos_qty_ty']) / sum(ship_main_new['pos_qty_ty']),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many product groups have 52 weeks in 2019? \n",
    "\n",
    "unique_product_group_2019 = ship_main_2019[ship_main_2019['week_of_year']<=32].groupby(['sell_id']).agg({'week_ending_date': lambda x: x.nunique()}).reset_index()\n",
    "print (\"Losing number of products with lifespan less than 32 weeks of data (%)\",round(1 - (len(unique_product_group_2019[unique_product_group_2019['week_ending_date']==32]) / len(unique_product_group_2019)),4)) \n",
    "\n",
    "#num_2019_critical_weeks = ship_main_2019['week_of_year']<32\n",
    "complete_2019 = unique_product_group_2019[unique_product_group_2019['week_ending_date']==32] \n",
    "incomplete_2019 = unique_product_group_2019[unique_product_group_2019['week_ending_date']<32] \n",
    "\n",
    "print (\"Losing 2019 revenue contribution (%) due to dropping products with less than 52 weeks of data\",round(1-sum(ship_main_2019.merge(complete_2019,on=['sell_id'],how='inner')['pos_dollar']) / sum(ship_main_2019['pos_dollar']),4)*100)\n",
    "print (\"Losing 2019 volume contribution (%) due to dropping products with less than 52 weeks of data\",round(1-sum(ship_main_2019.merge(complete_2019,on=['sell_id'],how='inner')['pos_qty']) / sum(ship_main_2019['pos_qty']),4)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products that satisfy the lifespan filter for both 2019 and 2020 - \n",
    "print (\"2020 products\",len(complete_2020))\n",
    "print (\"2019 products\",len(complete_2019))\n",
    "num_common =len(complete_2019.merge(complete_2020,how='inner',on=['sell_id']))\n",
    "num_not_common=len(ship_main_new['sell_id'].unique())-len(complete_2019.merge(complete_2020,how='inner',on=['sell_id']))\n",
    "print (\"Common products\",num_common)\n",
    "print (\"Non-common products\",num_not_common)\n",
    "print(f'Common rate {np.round(100*num_common/(num_common+num_not_common),2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products = complete_2019.merge(complete_2020,how='inner',on=['sell_id'])\n",
    "print (\"Losing 2020 & 2019 revenue contribution (%) due to dropping products with less than 52 weeks of 2019 data and 14 weeks of 2020 data\",round(1-sum(ship_main.merge(common_products,on=['sell_id'],how='inner')['pos_dollar']) / sum(ship_main['pos_dollar']),4))\n",
    "print (\"Losing 2020 & 2019 volume contribution (%) due to dropping products with less than 52 weeks of 2019 data and 14 weeks of 2020 data\",round(1-sum(ship_main.merge(common_products,on=['sell_id'],how='inner')['pos_qty']) / sum(ship_main['pos_qty']),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products['sell_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Prophet to run only common products - Only required when prophet is rerun after all states reopen\n",
    "# common_products[['sell_id']].to_parquet(\"./ship_model_data/wip/ship_common_products.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Modeling data for products with insufficient history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = common_products.merge(incomplete_2020,how='outer',on=['sell_id'])\n",
    "missing_products = list(set(ship_main_new['sell_id'].unique()) - set(one_df['sell_id'].unique()))\n",
    "missing_products_data = ship_main_new[ship_main_new['sell_id'].isin(missing_products)]\n",
    "incomplete_2020_data = ship_main_new[ship_main_new['sell_id'].isin(incomplete_2020['sell_id'].unique())]\n",
    "incomplete_data = incomplete_2020_data.append(missing_products_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Check - Number of unique products\")\n",
    "print (\"After all the business filters are applied and YoY calculation\",ship_main_new['sell_id'].nunique())\n",
    "print (\"Combination of modeling complete data, incomplete 2020 data, missing products\",len(common_products) + len(incomplete_2020) + len(missing_products))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incomplete_data['sell_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Model - Products with Insufficient History <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_ship_week = ship_main_new['week_ending_date'].max()\n",
    "date_list = projections_weekly.groupby(['week_ending_date','week_of_year']).agg({'state':'count'}).reset_index().sort_values('week_ending_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters \n",
    "#set of each each week in 2020\n",
    "weeks_2020 = set(ship_main_new['week_ending_date'].drop_duplicates())\n",
    "# Expected number of 2020 weeks\n",
    "num_weeks_expected = len(weeks_2020)\n",
    "# Forecast max date\n",
    "#max_forecast_date = '2020-08-08'\n",
    "# Max date we have in ship\n",
    "max_ship_date = str(max(weeks_2020).date())\n",
    "\n",
    "#Find sell_ids with incomplete data\n",
    "#sell_id_not_complete = unique_product_group[unique_product_group['week_ending_date']<num_weeks_expected]['sell_id'].unique()\n",
    "#TEST\n",
    "sell_id_not_complete = incomplete_data['sell_id'].unique()\n",
    "\n",
    "# Fill w 0’s\n",
    "#groupby df for iterating over sell_ids that are incomplete in 2020\n",
    "incomplete = ship_main_new[ship_main_new['sell_id'].isin(sell_id_not_complete)].groupby('sell_id')\n",
    "\n",
    "\n",
    "\n",
    "proj_dates = set(projections[(projections['date'].dt.weekday==5)&(projections['date']>max_ship_date)&(projections['date']<max_forecast_date)]['date'])\n",
    "proj_dates.add(pd.Timestamp(max_forecast_date))\n",
    "\n",
    "start_time = time.time()\n",
    "append_dfs_list = []\n",
    "print(f'modeling for {len(incomplete.groups.keys())} sell_ids with incomplete data')\n",
    "for sell_id in incomplete.groups.keys():\n",
    "    #Get sell_id data\n",
    "    data = incomplete.get_group(sell_id)\n",
    "    \n",
    "    #Find 2020 weeks where sell_id does not have an instance\n",
    "    weeks_missing = weeks_2020 - set(data['week_ending_date'])\n",
    "    #should be less than number of weeks in 2020\n",
    "    #assert len(weeks_missing)>0\n",
    "    \n",
    "    #Fill w 0’s\n",
    "    sell_id_missing_weeks_df1 = pd.DataFrame({'week_ending_date': list(weeks_missing)},columns=ship_main_new.columns)\n",
    "    sell_id_missing_weeks_df1['pos_qty_ty'].fillna(0,inplace=True)\n",
    "    sell_id_missing_weeks_df1['pos_dollar_ty'].fillna(0,inplace=True)\n",
    "    \n",
    "    sell_id_missing_weeks_df2 = pd.DataFrame({'week_ending_date': list(proj_dates)},columns=ship_main_new.columns)\n",
    "    sell_id_missing_weeks_df2['pos_qty_ty'].fillna(np.nan,inplace=True)\n",
    "    sell_id_missing_weeks_df2['pos_dollar_ty'].fillna(np.nan,inplace=True)\n",
    "    \n",
    "    sell_id_missing_weeks_df = pd.concat([sell_id_missing_weeks_df1,sell_id_missing_weeks_df2], axis=0, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    sell_id_missing_weeks_df['sell_id'].fillna(sell_id,inplace=True)\n",
    "    assert 0==sum(sell_id_missing_weeks_df['week_ending_date'].value_counts()>1)\n",
    "\n",
    "    #Should not print.\n",
    "    if len(sell_id_missing_weeks_df)==0: print(sell_id)\n",
    "    \n",
    "    #Append to make a list of df's that will be concated\n",
    "    append_dfs_list.append(sell_id_missing_weeks_df)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Number incomplete sell_ids modeled: {len(incomplete.groups.keys())}')\n",
    "print(\"--- Loop took %s minutes ---\",(end - start_time)/60)\n",
    "print(\"--- Time Per PPG: %s seconds ---\",(end - start_time)/len(sell_id_not_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast 4/11-8/8 with rolling mean per week\n",
    "missing_2020_ppgs_final = pd.concat(append_dfs_list, axis=0,ignore_index=True)\n",
    "missing_2020_ppgs_final['week_of_year'] = missing_2020_ppgs_final['week_ending_date'].dt.week\n",
    "missing_2020_ppgs_final.drop(columns=['pos_qty_ly'],inplace=True)\n",
    "missing_2020_ppgs_final = missing_2020_ppgs_final.merge(ship_main_2019[['sell_id','week_of_year','pos_qty']], on=['sell_id','week_of_year'],how='left',validate='1:1').rename(columns={'pos_qty':'pos_qty_ly'})                                                            \n",
    "\n",
    "sell_id_not_complete_all_data = pd.concat([ship_main_new[ship_main_new['sell_id'].isin(sell_id_not_complete)], missing_2020_ppgs_final],\\\n",
    "                       axis=0, ignore_index=True).sort_values(['sell_id','week_ending_date']).reset_index(drop=True)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "sell_id_not_complete_all_data['pos_qty_ly'].fillna(0, inplace=True)\n",
    "\n",
    "rolling_average_window = 7\n",
    "sell_id_not_complete_all_data['pos_qty_ly_rolling_6_week_mean'] = sell_id_not_complete_all_data[sell_id_not_complete_all_data['week_ending_date']!='2020-01-04'].groupby(['sell_id'])['pos_qty_ly'].rolling(window = rolling_average_window, center=True, min_periods=1).mean().reset_index(0,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['state', 'promoted_product_group_desc', 'product_segment_name', 'retailer_desc','channel','retailer','mdlz_business','ppg_id','mdlz_category','mdlz_brand','mdlz_ppg']:\n",
    "    sell_id_not_complete_all_data[col]=sell_id_not_complete_all_data.groupby('sell_id')[col].fillna(method='ffill')\n",
    "    sell_id_not_complete_all_data[col]=sell_id_not_complete_all_data.groupby('sell_id')[col].fillna(method='bfill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_covid_2020 = ship_main_new[ship_main_new['week_ending_date']<'2020-03-01']\n",
    "non_covid_2020['pos_qty_diff'] = (non_covid_2020['pos_qty_ty']-non_covid_2020['pos_qty_ly'])/non_covid_2020['pos_qty_ly']\n",
    "non_covid_2020 = non_covid_2020.groupby(['sell_id'])['pos_qty_diff'].median().reset_index()\n",
    "non_covid_2020.rename(columns={'pos_qty_diff':'2020_med_pos_qty_diff'},inplace=True)\n",
    "sell_id_not_complete_all_data = sell_id_not_complete_all_data.merge(non_covid_2020, on=['sell_id'], how='left', validate='m:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sell_id_not_complete_all_data[sell_id_not_complete_all_data['sell_id']=='ALL OTHER BELVITA COOKIE PPG_Belvita Base_AWG SCBM_45_Total Biscuit MG_Cookies_Belvita Cookies_L90_MS_US3000030'].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sell_id_not_complete_all_data['2020_med_pos_qty_diff'].isna().mean())\n",
    "sell_id_not_complete_all_data['2020_med_pos_qty_diff']=sell_id_not_complete_all_data.groupby(['sell_id'])['2020_med_pos_qty_diff'].fillna(method='pad')\n",
    "sell_id_not_complete_all_data['2020_med_pos_qty_diff'].fillna(0,inplace=True)\n",
    "sell_id_not_complete_all_data['2020_med_pos_qty_diff'].replace(np.inf, 0, inplace=True)\n",
    "sell_id_not_complete_all_data['2020_med_pos_qty_diff'].replace(np.nan, 0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_id_not_complete_all_data['forecast_quantity'] = sell_id_not_complete_all_data['pos_qty_ly_rolling_6_week_mean']\n",
    "sell_id_not_complete_all_data['forecast_quantity'].fillna(0, inplace=True)\n",
    "\n",
    "#Set null where we have data\n",
    "sell_id_not_complete_all_data.loc[sell_id_not_complete_all_data['week_ending_date']<max_ship_date, 'forecast_quantity'] = np.nan\n",
    "\n",
    "# Fill max ship date with actual. For viz\n",
    "sell_id_not_complete_all_data.loc[sell_id_not_complete_all_data['week_ending_date']==max_ship_date,'forecast_quantity']=sell_id_not_complete_all_data.loc[sell_id_not_complete_all_data['week_ending_date']==max_ship_date,'pos_qty_ty']\n",
    "sell_id_not_complete_all_data['common_product']=0\n",
    "\n",
    "ship_incomplete = sell_id_not_complete_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_forecast(week,avg,qty_ty,qty_ly,state):\n",
    "    \n",
    "    if week==latest_ship_week:\n",
    "        return qty_ty\n",
    "    elif week>latest_ship_week and state!='':\n",
    "        return avg\n",
    "    elif week>latest_ship_week and state=='': #Null state forecast\n",
    "        return qty_ly\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Milestone Modeling + Prophet Model - Products with Complete POS History <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Modeling Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_model_data_raw = ship_main_new.merge(common_products['sell_id'],on=['sell_id'],how='inner')\n",
    "ship_model_data = ship_model_data_raw.copy()\n",
    "ship_model_data.drop(['retailer', 'promoted_product_group_desc', 'product_segment_name', 'retailer_desc','channel','mdlz_business','mdlz_category','mdlz_brand','mdlz_ppg','ppg_id','pos_qty_ly','pos_dollar_ly'],axis=1,inplace=True)\n",
    "\n",
    "ship_model_data['state_null'] = np.where(ship_model_data.state.isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataset on just the peak covid growth windows (3/14 & 3/21)\n",
    "maximum_peak_data = ship_model_data[(ship_model_data['week_of_year']>=11) & (ship_model_data['week_of_year']<=12)]\n",
    "\n",
    "# For each sell id, Identify the growth peak corresponding to those two weeks\n",
    "sell_maxium_peak = maximum_peak_data.loc[maximum_peak_data.groupby('sell_id')['Growth_perc_qty'].idxmax()][['sell_id','Growth_perc_qty']]\n",
    "sell_maxium_peak.rename(columns={'Growth_perc_qty':'covid_max_growth'},inplace=True)\n",
    "\n",
    "ship_model_data = ship_model_data.merge(sell_maxium_peak,on=['sell_id'],how='left')\n",
    "ship_model_data['negative_covid_impact'] = ship_model_data['covid_max_growth'].apply(lambda x: 1 if x<0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating 6 week rolling average after sorting the 2019 dataframe\n",
    "ship_main_2019 = ship_main_2019.sort_values(['sell_id','week_of_year'])\n",
    "rolling_average_window = 6\n",
    "ship_main_2019['pos_qty_rolling_6_week_med'] = ship_main_2019[ship_main_2019['week_ending_date']!='2020-01-04']\\\n",
    "                                                .groupby(['sell_id'])['pos_qty']\\\n",
    "                                                .rolling(window = rolling_average_window).median()\\\n",
    "                                                .reset_index(0,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nship Model data Summary\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(ship_model_data)))\n",
    "print (\"\\nNumber of unique products: {0:,.0f}\".format(len(ship_model_data['sell_id'].unique())))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(ship_model_data['pos_dollar_ty'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(ship_model_data['pos_qty_ty'])))\n",
    "print (\"\\nRevenue contribution (%) lost due to the product filters\",round((1 - (np.nansum(ship_model_data['pos_dollar_ty'])/np.nansum(ship_main_2020['pos_dollar']))),3))\n",
    "print (\"\\nVolume contribution (%) lost due to the product filters\",round((1 - (np.nansum(ship_model_data['pos_qty_ty'])/np.nansum(ship_main_2020['pos_qty']))),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable declarations for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of future weeks to be projected \n",
    "date_list = projections_weekly.groupby(['week_ending_date','week_of_year']).agg({'state':'count'}).reset_index().sort_values('week_ending_date')\n",
    "latest_ship_week = ship_main_2020['week_ending_date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_decay_multiplier(growth):\n",
    "    \n",
    "    if growth<0:\n",
    "        return 0.8\n",
    "    \n",
    "    elif growth*100 >= 0 and growth*100 <= 50:\n",
    "        return 0.2\n",
    "    \n",
    "    elif growth*100 > 50 and growth*100 <= 100:\n",
    "        return 0.15\n",
    "    \n",
    "    elif growth*100 > 100 and growth*100 <= 500:\n",
    "        return 0.1\n",
    "    \n",
    "    elif growth*100 > 500 and growth*100 <= 1000:\n",
    "        return 0.08\n",
    "    \n",
    "    else:\n",
    "        return 0.05       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe with just the headers \n",
    "ship_merged = pd.DataFrame(columns=['week_ending_date','sell_id','state','pos_qty_ty','pos_dollar_ty','week_of_year_x','Growth_perc_sales'\n",
    "                                  ,'Growth_perc_qty','week_of_year_y','first_milestone_date','second_milestone_date','third_milestone_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product_groups = dataset[dataset['state_null']==0].groupby(['sell_id','state'])\n",
    "product_groups = ship_model_data[ship_model_data['state_null']==0].groupby(['sell_id','state'])\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Modeling {len(product_groups.groups.keys())} unique sell_ids')\n",
    "for key in product_groups.groups.keys():\n",
    "    \n",
    "    data = product_groups.get_group(key).reset_index() # 14 week actuals dataframe for each product group\n",
    "    \n",
    "    state_value = key[1]\n",
    "    state_null_value = data['state_null'].iat[0]\n",
    "    covid_impact_value = data['negative_covid_impact'].iat[0]\n",
    "    \n",
    "    # To get future weeks for each unique product group\n",
    "    data = data.merge(date_list[['week_ending_date','week_of_year']], on=['week_ending_date'],how ='outer') \n",
    "    \n",
    "    # Fill in sell_id and state values for the future forecasting weeks \n",
    "    data['sell_id'] = data[\"sell_id\"].fillna(key[0])\n",
    "    data['state'] = data[\"state\"].fillna(key[1])\n",
    "    data['state_null'] = data[\"state_null\"].fillna(state_null_value)\n",
    "    data['negative_covid_impact'] = data[\"negative_covid_impact\"].fillna(covid_impact_value)\n",
    "        \n",
    "    \n",
    "    ############################################ Growth Decay Modeling ######################################################\n",
    "    \n",
    "    death_peak_date = projections_dates[projections_dates['Geography'] == 'United States of America']['peak_deaths_week'].iat[0]\n",
    "    death_thirty_perc_date = projections_dates[projections_dates['state'] == state_value]['thirtyperc_deaths_week'].iat[0]\n",
    "    \n",
    "    # Before covid outbreak median & average growth\n",
    "    median = data[(data['week_ending_date']>='2020-01-01') & (data['week_ending_date'] <'2020-03-01')]['Growth_perc_qty'].median() \n",
    "    average = data[(data['week_ending_date']>='2020-01-01') & (data['week_ending_date'] <'2020-03-01')]['Growth_perc_qty'].mean()\n",
    "    \n",
    "    # Find the absolute peak growth value and it's corresponding week \n",
    "    peak_growth_value = data.loc[data[(data['week_ending_date']>=growth_peak_dates[0]) & (data['week_ending_date']<=growth_peak_dates[1])]['Growth_perc_qty'].idxmax()]['Growth_perc_qty']\n",
    "    peak_growth_date = data.loc[data[(data['week_ending_date']>=growth_peak_dates[0]) & (data['week_ending_date']<=growth_peak_dates[1])]['Growth_perc_qty'].idxmax()]['week_ending_date']\n",
    "    \n",
    "    \n",
    "    data['forecast1'] = 0.0\n",
    "    data['forecast_quantity'] = 0.0\n",
    "\n",
    "    \n",
    "    # Replicate the peak growth value as the very first forecast value - just for visualization (including stop gap measure)\n",
    "    latest_growth_value = data.loc[data['week_ending_date'] == latest_ship_week]['Growth_perc_qty'].iat[0]\n",
    "    latest_ship_value = data.loc[data['week_ending_date'] == latest_ship_week]['pos_qty_ty'].iat[0]\n",
    "    \n",
    "#     data.loc[(data['week_ending_date'] >= latest_ship_week) & (data['week_ending_date'] < death_peak_date),'forecast1'] = latest_growth_value\n",
    "#     data.loc[(data['week_ending_date'] >= latest_ship_week) & (data['week_ending_date'] < death_peak_date),'forecast_quantity'] = latest_ship_value\n",
    "    data.loc[(data['week_ending_date'] == latest_ship_week),'forecast1'] = latest_growth_value\n",
    "    data.loc[(data['week_ending_date'] == latest_ship_week),'forecast_quantity'] = latest_ship_value\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###### `````````````````````````````````````````````` First Milestone Period ``````````````````````````````````````````````````######\n",
    "    \n",
    "    first_milestone_date = death_peak_date #US specific \n",
    "    # Check if the first two milestones co-incide\n",
    "    if death_thirty_perc_date <= death_peak_date:\n",
    "        second_milestone_date = death_peak_date + timedelta(days=21) #Based on median days being 28 across states\n",
    "    else:\n",
    "        second_milestone_date = death_thirty_perc_date\n",
    "    \n",
    "    first_milestone_date = first_milestone_date + timedelta(days=7) # Shifting the range by one week because the latest pos week conincides with the US deaths peak date\n",
    "    ### Optimization to create a prior table,dictionary based on sell_id, growth_value \n",
    "    data.loc[(data['week_ending_date'] == first_milestone_date),'forecast1'] = growth_decay_multiplier(peak_growth_value) * peak_growth_value\n",
    "    if data.loc[data['week_ending_date'] == first_milestone_date]['negative_covid_impact'].iat[0]==0:\n",
    "        data.loc[(data['week_ending_date'] == first_milestone_date),'forecast_quantity'] = 0.9 * latest_ship_value\n",
    "    else: \n",
    "        data.loc[(data['week_ending_date'] == first_milestone_date),'forecast_quantity'] = latest_ship_value / 0.9 \n",
    "    second_milestone_date_updated = second_milestone_date + timedelta(days=7)\n",
    "    first_milestone_range = data.loc[(data['week_ending_date'] >= first_milestone_date) & (data['week_ending_date']< second_milestone_date_updated),'forecast1'].index.values\n",
    "    for i in range(first_milestone_range[0], first_milestone_range[-1]+1): \n",
    "        data.loc[i+1, 'forecast1'] = data.loc[i, 'forecast1'] * 0.9 \n",
    "        if data.loc[i,'negative_covid_impact']==0:\n",
    "            data.loc[i+1, 'forecast_quantity'] = data.loc[i, 'forecast_quantity'] * 0.9\n",
    "        else:\n",
    "            data.loc[i+1, 'forecast_quantity'] = data.loc[i, 'forecast_quantity'] / 0.9\n",
    "\n",
    "\n",
    "    ######`````````````````````````````````````````````` Second Milestone Period ``````````````````````````````````````````````######\n",
    "    state_open_date = second_milestone_date_updated + timedelta(days=14) \n",
    "    state_open_date = pd.to_datetime(state_open_date)\n",
    "    second_milestone_range = data.loc[(data['week_ending_date'] >= second_milestone_date_updated) & (data['week_ending_date']< state_open_date),'forecast1'].index.values\n",
    "    \n",
    "    data.loc[second_milestone_range[0]:second_milestone_range[0]+1, 'forecast_quantity'] = 0   # Setting this as 0 in the first milestone range = 1 week only\n",
    "    for i in range(second_milestone_range[0], second_milestone_range[-1]+1):\n",
    "        data.loc[i+1, 'forecast1'] = data.loc[i, 'forecast1'] * 0.5\n",
    "\n",
    "    \n",
    "    #######`````````````````````````````````````````````` Third Milestone Period ``````````````````````````````````````````````#######\n",
    "    data.loc[(data['week_ending_date']>= state_open_date),'forecast1'] = median\n",
    "    \n",
    "    data['first_milestone_date'] = death_peak_date\n",
    "    data['second_milestone_date'] = second_milestone_date\n",
    "    data['third_milestone_date'] = state_open_date\n",
    "    data['median_baseline'] = median\n",
    "    \n",
    "    ship_merged = ship_merged.append(data)\n",
    "\n",
    "end = time.time()\n",
    "print(\"--- %s minutes ---\",(end - start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing of the Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Prohpet's forecasts\n",
    "prht_fcst_common = pd.read_feather(f\"./ship_prophet_results/0430_cleaned/ship_0430_cleaned_v2.feather\")\n",
    "\n",
    "# Calculate rolling 6 week median of prophet's forecast to exclude effect of promotions\n",
    "prht_fcst_common = prht_fcst_common.sort_values(['sell_id','week_ending_date'])\n",
    "rolling_average_window = 6\n",
    "prht_fcst_common['pos_qty_prht_rolling_6_week_med'] = prht_fcst_common.groupby(['sell_id'])['prht_frcst'].rolling(window = rolling_average_window).median().reset_index(0,drop=True)\n",
    "\n",
    "prht_fcst_common = prht_fcst_common.drop(['prht_frcst'], axis=1)\n",
    "\n",
    "# prht_fcst_common.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the model output df - dont use it, overwritten by ship_merged\n",
    "ship_merged_raw = ship_merged.copy()\n",
    "\n",
    "### ship-Processing of the Model Output\n",
    "ship_merged = ship_merged.drop(['week_of_year_x','index','covid_max_growth'],axis=1)\n",
    "ship_merged.rename(columns={'week_of_year_y':'week_of_year'},inplace=True)\n",
    "ship_merged['common_product']=1\n",
    "\n",
    "# Merge model output with 2019 dataset to obtain the pos_qty_ly\n",
    "ship_merged = ship_merged.merge(ship_main_2019,on=['sell_id','week_of_year'],how='left',suffixes=('', '_y'))\n",
    "ship_merged.rename(columns={'pos_qty':'pos_qty_ly','pos_dollar':'pos_dollar_ly'},inplace=True)\n",
    "ship_merged.drop(['week_ending_date_y','state_y','year'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Merge in prophet's forecast for common products\n",
    "ship_merged = ship_merged.merge(prht_fcst_common, how='left', on=['sell_id','week_ending_date'])\n",
    "\n",
    "# Use prophet's forecast if available\n",
    "ship_merged = ship_merged.assign(pos_qty_rolling_6_week_med = \\\n",
    "                              np.where(~(ship_merged.pos_qty_prht_rolling_6_week_med.isnull()), \n",
    "                                       ship_merged.pos_qty_prht_rolling_6_week_med, \n",
    "                                       ship_merged.pos_qty_rolling_6_week_med))\n",
    "\n",
    "ship_merged = ship_merged.drop(['pos_qty_prht_rolling_6_week_med'],axis=1)\n",
    "\n",
    "\n",
    "# Subset the dataset on just the peak covid growth windows (3/14 & 3/21)\n",
    "maximum_peak_data = ship_merged[(ship_merged['week_of_year']>=11) & (ship_merged['week_of_year']<=12)]\n",
    "\n",
    "# For each sell id, Identify the growth peak corresponding to those two weeks\n",
    "sell_maxium_ly = maximum_peak_data.loc[maximum_peak_data.groupby('sell_id')['Growth_perc_qty'].idxmax()][['sell_id','pos_qty_ly']]\n",
    "sell_maxium_ly.rename(columns={'pos_qty_ly':'max_ship_ly'},inplace=True)\n",
    "\n",
    "ship_merged = ship_merged.merge(sell_maxium_ly,on=['sell_id'],how='left')\n",
    "\n",
    "def apply_forecast_value_positive(week,growth,qty_ly_rolling,qty_ty,qty_ly_max,z,third_m,second_m,fcst_qty,high_growth_decline):\n",
    "      \n",
    "    if week == latest_ship_week: # If the week is current ship week, set the forecast value to actual ship (just for viz purporse)\n",
    "        return qty_ty\n",
    "    \n",
    "    elif week>latest_ship_week and week<second_m:\n",
    "        return fcst_qty\n",
    "    \n",
    "    elif week>=second_m and high_growth_decline==True:\n",
    "        return qty_ly_rolling\n",
    "    elif week>=second_m:\n",
    "        return (1 + growth) * qty_ly_rolling    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def apply_forecast_value_negative(week,growth,qty_ly_rolling,qty_ty,qty_ly_max,z,third_m,second_m,fcst_qty,high_growth_decline):\n",
    "      \n",
    "    if week == latest_ship_week: # If the week is current ship week, set the forecast value to actual ship (just for viz purporse)\n",
    "        return qty_ty\n",
    "    \n",
    "    elif week>latest_ship_week and week<second_m:\n",
    "        return fcst_qty\n",
    "    \n",
    "    elif week>=second_m and week<third_m:\n",
    "        return (1 + growth) * qty_ly_max    \n",
    "    \n",
    "    elif week>=third_m and high_growth_decline==True:\n",
    "        return qty_ly_rolling\n",
    "    elif week>=third_m:\n",
    "        return (1+growth) * qty_ly_rolling\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def floor_growth(median,growth,week,z): # positive growing products\n",
    "        \n",
    "    if week>latest_ship_week and growth<median:\n",
    "        return median\n",
    "    else:\n",
    "        return growth\n",
    "\n",
    "def ceil_growth(median,growth,week,z): # Negative growing products\n",
    "    \n",
    "    if week>latest_ship_week and growth>median:\n",
    "        return median\n",
    "    else:\n",
    "        return growth\n",
    "\n",
    "ship_merged['forecast1'] = ship_merged[['median_baseline','forecast1','week_ending_date','negative_covid_impact']].apply(lambda x: floor_growth(*x) if x[3] == 0 else ceil_growth(*x),axis=1)\n",
    "ship_merged['forecast_quantity'] = ship_merged[['week_ending_date','forecast1','pos_qty_rolling_6_week_med',\n",
    "                                                'pos_qty_ty','max_ship_ly','negative_covid_impact',\n",
    "                                                'third_milestone_date','second_milestone_date',\n",
    "                                                'forecast_quantity','high_growth_decline']]\\\n",
    "                                    .apply(lambda x: apply_forecast_value_positive(*x) if x[5]==0  \\\n",
    "                                            else apply_forecast_value_negative(*x) ,axis=1)\n",
    "\n",
    "ship_merged['week_ending_date'] = pd.to_datetime(ship_merged['week_ending_date'])\n",
    "ship_merged['first_milestone_date'] = pd.to_datetime(ship_merged['first_milestone_date'])\n",
    "ship_merged['second_milestone_date'] = pd.to_datetime(ship_merged['second_milestone_date'])\n",
    "ship_merged['third_milestone_date'] = pd.to_datetime(ship_merged['third_milestone_date'])\n",
    "\n",
    "\n",
    "\n",
    "def update_forecast_positive(week,forecast1,fcst_growth,pos_qty_rolling,fcst_qty,first_m,second_m,neg_covid, high_growth_decline):\n",
    "\n",
    "    if week>latest_ship_week and week<second_m and high_growth_decline!=True: # Floor the value \n",
    "        if fcst_growth < forecast1:\n",
    "            return (1+forecast1) * pos_qty_rolling\n",
    "        else:\n",
    "            return fcst_qty\n",
    "    \n",
    "    else:\n",
    "        return fcst_qty\n",
    "\n",
    "def update_forecast_negative(week,forecast1,fcst_growth,pos_qty_rolling,fcst_qty,first_m,second_m,neg_covid, high_growth_decline):\n",
    "\n",
    "    if week>latest_ship_week and week<second_m and high_growth_decline!=True: # Ceil the value\n",
    "        if fcst_growth > forecast1:\n",
    "            return (1+forecast1) * pos_qty_rolling\n",
    "        else:\n",
    "            return fcst_qty\n",
    "    \n",
    "    else:\n",
    "        return fcst_qty\n",
    "\n",
    "#'week_ending_date','second_milestone_date_y','pos_qty_ly','pos_qty_rolling_6_week_med'\n",
    "def find_promo_uplift(week,second_m,qty_ly,qty_rolling):\n",
    "    if week>=(pd.to_datetime(max_train_date) + timedelta(days=14)):\n",
    "        if (qty_ly - qty_rolling)>0:\n",
    "            return qty_ly - qty_rolling\n",
    "        else: \n",
    "            return 0 \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_merged['forecast_growth'] = (ship_merged['forecast_quantity'] - ship_merged['pos_qty_ly']) / (ship_merged['pos_qty_ly'])\n",
    "\n",
    "ship_merged['forecast_quantity_new'] = ship_merged[['week_ending_date','forecast1','forecast_growth',\n",
    "                                                    'pos_qty_rolling_6_week_med','forecast_quantity',\n",
    "                                                    'first_milestone_date','second_milestone_date',\n",
    "                                                    'negative_covid_impact','high_growth_decline']]\\\n",
    "                                            .apply(lambda x: update_forecast_positive(*x) if x[7]==0 \n",
    "                                                   else update_forecast_negative(*x) ,axis=1)\n",
    "\n",
    "ship_merged.drop(['forecast_quantity'],axis=1,inplace=True)\n",
    "ship_merged.rename(columns={'forecast_quantity_new':'forecast_quantity'},inplace=True)\n",
    "\n",
    "# Save a copy for backup! 4/16 at 9:36 pm (negative and positive logic)\n",
    "ship_merged_copy = ship_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After new logic \n",
    "ship_merged_copy_new_logic = ship_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ship_merged['pos_qty_ly'].sum()+ship_incomplete['pos_qty_ly'].sum())\n",
    "\n",
    "ship_incomplete.rename(columns={'pos_qty_ly_rolling_6_week_mean':'pos_qty_rolling_6_week_med'},inplace=True)\n",
    "\n",
    "ship_merged = pd.concat([ship_merged, ship_incomplete], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products = ship_merged[ship_merged['common_product']==1]['sell_id'].unique()\n",
    "print('common products (all 2019+2020) weeks available')\n",
    "print('sell ids', np.round(100*len(common_products)/len(ship_main['sell_id'].unique()),2),'%')\n",
    "print('pos qty', np.round(100*ship_main[ship_main['sell_id'].isin(common_products)]['pos_qty'].sum()/ship_main['pos_qty'].sum(),2),'%')\n",
    "print()\n",
    "\n",
    "incomplete2020_products= ship_merged[ship_merged['common_product']==0]['sell_id'].unique()\n",
    "print('non-common 2020 products (not max 2020 weeks available)')\n",
    "print('sell ids', np.round(100*len(incomplete2020_products)/len(ship_main['sell_id'].unique()),2),'%')\n",
    "print('pos qty', np.round(100*ship_main[ship_main['sell_id'].isin(incomplete2020_products)]['pos_qty'].sum()/ship_main['pos_qty'].sum(),2),'%')\n",
    "print()\n",
    "      \n",
    "\n",
    "not_modeled_df = ship_main[~ship_main['sell_id'].isin(ship_merged['sell_id'].unique())]\n",
    "not_modeled = ship_main[~ship_main['sell_id'].isin(ship_merged['sell_id'].unique())]['sell_id'].unique()\n",
    "print('Not Modeled')\n",
    "print('sell ids', np.round(100*len(not_modeled)/len(ship_main['sell_id'].unique()),2),'%')\n",
    "print('pos qty', np.round(100*ship_main[ship_main['sell_id'].isin(not_modeled)]['pos_qty'].sum()/ship_main['pos_qty'].sum(),2),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dates_df = pd.DataFrame(columns = ['state','first_milestone_date', 'second_milestone_date','third_milestone_date'])\n",
    "death_peak_date = projections_dates[projections_dates['Geography'] == 'United States of America']['peak_deaths_week'].iat[0]\n",
    "state_dates_df=[]\n",
    "for state in projections_dates[projections_dates['state'].notna()]['state'].unique():\n",
    "    death_thirty_perc_date = projections_dates[projections_dates['state'] == state]['thirtyperc_deaths_week'].iat[0]\n",
    "    #if state == 'NY':\n",
    "    #    print(death_thirty_perc_date)\n",
    "    #    print(death_peak_date)\n",
    "    if death_thirty_perc_date <= death_peak_date:\n",
    "        second_milestone_date = death_peak_date + timedelta(days=21) #Based on median days being 28 across states\n",
    "    else:\n",
    "        second_milestone_date = death_thirty_perc_date\n",
    "    \n",
    "    state_open_date = second_milestone_date + timedelta(days=14) \n",
    "\n",
    "    \n",
    "    state_dates_df.append([state,death_peak_date,second_milestone_date,state_open_date])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dates_df = pd.DataFrame(state_dates_df,columns = ['state','first_milestone_date', 'second_milestone_date','third_milestone_date'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_merged = ship_merged.merge(state_dates_df, how='left', on='state', validate = 'm:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FS Constant growth until third milestone\n",
    "\n",
    "ship_merged.loc[ship_merged['week_ending_date']==max_train_date ,'last_ship_week_pos_qty'] = ship_merged['pos_qty_ty']\n",
    "ship_merged['last_ship_week_pos_qty']=ship_merged.groupby(['sell_id'])['last_ship_week_pos_qty'].fillna(method='pad')\n",
    "\n",
    "ship_merged.loc[((ship_merged['common_product']==0)&\\\n",
    "                 (ship_merged['channel']=='30')&\\\n",
    "                 (ship_merged['week_ending_date']>=max_train_date) & \\\n",
    "                 (ship_merged['week_ending_date']<ship_merged['third_milestone_date_y']) ), \n",
    "                'forecast_quantity'] = ship_merged['last_ship_week_pos_qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncommon_promo_damp(s):\n",
    "    s.sort_values(['sell_id', 'week_ending_date'], ascending=[True,True], inplace=True)\n",
    "    print('CAUTION: sorting shipments dataframe by sell_id and time: early -> late')\n",
    "    s.loc[s['pos_qty_ly']==0, 'zero_median']=1\n",
    "    s['7_window_0_count']=s.groupby('sell_id')['zero_median'].apply(lambda x: x.rolling(7, center=True).count())\n",
    "    s['uncommon_dampen_multiplier'] = 1-s['7_window_0_count']/7\n",
    "    s.loc[s['common_product']==1, 'uncommon_dampen_multiplier']=1\n",
    "    s['promo_uplift_new'] = s['promo_uplift'] * s['uncommon_dampen_multiplier']\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Ecomm\n",
    "ship_merged.loc[((ship_merged['common_product']==0)&(ship_merged['week_ending_date']>max_train_date)&(ship_merged['retailer_desc']=='ECOMM DIRECT')), 'forecast_quantity'] = ship_merged['forecast_quantity']*2\n",
    "ship_merged.loc[((ship_merged['common_product']==1)&(ship_merged['week_ending_date']>max_train_date)&(ship_merged['retailer_desc']=='ECOMM DIRECT')), 'forecast_quantity'] = ship_merged['forecast_quantity']*2\n",
    "\n",
    "#Promo uplift - to review\n",
    "ship_merged['promo_uplift'] = ship_merged[['week_ending_date','second_milestone_date_y','pos_qty_ly','pos_qty_rolling_6_week_med']].apply(lambda x: find_promo_uplift(*x),axis=1)\n",
    "\n",
    "# Corrections for DOT and McLane since they only capture returns in negatives\n",
    "ship_merged.loc[(ship_merged['state'].isin(['AR','IL']))&(ship_merged['retailer_desc'].isin(['DOT TOTAL'])), 'forecast_quantity'] = ship_merged['pos_qty_ly']\n",
    "ship_merged.loc[(ship_merged['state'].isin(['IL']))&(ship_merged['retailer_desc'].isin(['MCLANE'])), 'forecast_quantity'] = ship_merged['pos_qty_ly']\n",
    "ship_merged=ship_merged[ship_merged['forecast_quantity']!=np.inf]\n",
    "ship_merged.loc[ship_merged['week_ending_date']<max_train_date, 'forecast_quantity'] = np.nan\n",
    "\n",
    "\n",
    "# Promo uplift reduction for uncommon products\n",
    "common_promo_mean = ship_merged[ship_merged['common_product']==1]['promo_uplift'].mean()\n",
    "ship_merged = uncommon_promo_damp(ship_merged)\n",
    "assert common_promo_mean == ship_merged[ship_merged['common_product']==1]['promo_uplift'].mean()\n",
    "\n",
    "# Uncommon products\n",
    "#ship_merged.loc[~ship_merged['sell_id'].isin(ship_incomplete['sell_id'].unique()),'common_product']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Output <a class=\"anchor\" id=\"seventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Naive and Milestone Modeling outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Without merging with the states data, number of rows\", len(ship_merged))\n",
    "print (\"Without merging with the states data, number of product groups\", len(ship_merged[\"sell_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Write out Model output parquet file to edge node\n",
    "start = time.time()\n",
    "\n",
    "out_buffer = BytesIO()\n",
    "ship_merged.to_parquet(out_buffer, index=False)\n",
    "\n",
    "destination_path = \"ship_model_results.parquet.gzip\"\n",
    "# destination_path = \"ship_model_results.csv\"\n",
    "with edge_node.open(destination_path, 'w+', 32768) as f:\n",
    "    f.write(out_buffer.getvalue())\n",
    "\n",
    "print('Writing out took', time.time()-start, 'seconds.')\n",
    "\n",
    "# Add edge node to Hadoop connection\n",
    "edge_node_pwd = edge_node.execute(\"pwd\")[0][:-1].decode(\"utf-8\")\n",
    "edge_node.execute(f\"hdfs dfs -put {edge_node_pwd}/{destination_path} ./CBDA/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
