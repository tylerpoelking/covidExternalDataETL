{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('_CBDA_Ship_1_ETL_Preprocessing.json', 'r') as f:\n",
    "    r = f.readline()\n",
    "    \n",
    "r = r.encode().decode('utf-8-sig')\n",
    "\n",
    "j = json.loads(r)\n",
    "\n",
    "for cell in j['paragraphs']:\n",
    "    cell_text = cell['text']\n",
    "    print(cell_text, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "print(spark.version)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# %pyspark\n",
    "spark.sql(\"use d4sa_us_disc\")\n",
    "\n",
    "# %pyspark\n",
    "\n",
    "# Apply filters to discovery table so that the numbers align with publish tables that MDLZ stakeholders use for reporting / analysis\n",
    "raw_shipments_curr = spark.table(\"d4sa_us_disc.fact_spark56_daily_int_00_001\").\\\n",
    "filter(\"D_BIC_ZMOD_IND = 'ST'\").\\\n",
    "filter(\"D_BIC_ZPHIER03 != ''\").\\\n",
    "filter(\"D_VERSION == '000'\").\\\n",
    "filter(\"(D_BILL_DATE != '00000000') OR (D_BILL_DATE == '00000000' AND D_BIC_ZINFOPROV == 'ZSCNNC53')\").\\\n",
    "filter(\"D_BIC_ZCAHIER01 != 'US1000012'\").\\\n",
    "filter(\"D_ZCAHIER01_T != ''\").\\\n",
    "filter(\"D_BIC_ZG_AVV004 != 0\")\n",
    "# Filter for only 2020 data from this table since 2019 data (esp earlier weeks) seem incomplete\n",
    "raw_shipments_curr = raw_shipments_curr.filter(\"D_FISCPER IN ('2020001','2020002','2020003','2020004','2020005','2020006','2020007','2020008','2020009','2020010','2020011','2020012')\")\n",
    "\n",
    "raw_shipments_hist = spark.table(\"d4sa_us_disc.fact_spark56_daily_hist_int_00_001\").\\\n",
    "filter(\"D_BIC_ZMOD_IND = 'ST'\").\\\n",
    "filter(\"D_BIC_ZPHIER03 != ''\").\\\n",
    "filter(\"D_VERSION == '000'\").\\\n",
    "filter(\"(D_BILL_DATE != '00000000') OR (D_BILL_DATE == '00000000' AND D_BIC_ZINFOPROV == 'ZSCNNC53')\").\\\n",
    "filter(\"D_BIC_ZCAHIER01 != 'US1000012'\").\\\n",
    "filter(\"D_ZCAHIER01_T != ''\").\\\n",
    "filter(\"D_BIC_ZG_AVV004 != 0\")\n",
    "raw_shipments_hist = raw_shipments_hist.filter(\"D_FISCPER IN ('2019001','2019002','2019003','2019004','2019005','2019006','2019007','2019008','2019009','2019010','2019011','2019012')\")\n",
    "\n",
    "# Append the 2020 curr and 2019 hist DFs\n",
    "raw_shipments_hist = raw_shipments_hist.select(*raw_shipments_curr.columns)\n",
    "raw_shipments = raw_shipments_curr.union(raw_shipments_hist)\n",
    "\n",
    "raw_shipments.createOrReplaceTempView(\"fact\")\n",
    "\n",
    "# DO NOT USE - filter(\"D_BIC_ZCGHIER02 NOT IN ('G20000003','G20000008')\")\n",
    "\n",
    "# %pyspark\n",
    "\n",
    "# Read cust hiererachy data\n",
    "cust_hier = spark.table(\"hierarchies_customer_curr_hierarchy_int_00_001\")\n",
    "\n",
    "# Rename Dist channel ID column\n",
    "cust_hier = cust_hier.withColumnRenamed(\"distribution_channel_id\", \"distribution_channel_id_cust\")\n",
    "\n",
    "# pad 0s to store_id col\n",
    "cust_hier = cust_hier.withColumn(\"store_id\" , lpad(cust_hier['store_id'],10,'0').alias('store_id'))\n",
    "cust_hier.createOrReplaceTempView(\"hierarchies_customer\")\n",
    "\n",
    "# Filter for gc_area_id not null - Use an inner join with fact to filter out there as well\n",
    "#cust_hier = cust_hier.filter(\"gc_area_id IS NOT NULL\")\n",
    "\n",
    "z.show(cust_hier)\n",
    "\n",
    "# %pyspark\n",
    "\n",
    "prod_hier = spark.table(\"hierarchies_product_curr_hierarchy_int_00_001\")\n",
    "# Rename Dist channel ID column\n",
    "prod_hier = prod_hier.withColumnRenamed(\"distribution_channel_id\", \"distribution_channel_id_prod\")\n",
    "# pad 0s to SKU_Id col\n",
    "prod_hier = prod_hier.withColumn(\"sku_id\" , lpad(prod_hier['sku_id'],18,'0').alias('sku_id'))\n",
    "\n",
    "prod_hier.createOrReplaceTempView(\"hierarchies_product\")\n",
    "\n",
    "z.show(prod_hier.limit(10))\n",
    "\n",
    "# %pyspark\n",
    "query = \"\"\"\n",
    "select g.date0, g.fiscper, g.promoted_product_group, g.promoted_product_group_desc, g.product_family_desc, g.product_segment_name, g.product_category_name, g.management_grouping_desc, g.state, g.ac_scbm_id, g.ac_scbm_desc, g.bic_zdistr_ch, g.state, sum(bic_zg_avv004) as gross_sales, sum(bic_znt_wt_lb) as sales_pounds\n",
    "from \n",
    "(select *\n",
    " from (select D_BIC_ZFISCDAY as date0, D_FISCPER as fiscper, D_CUSTOMER as customer, D_MATERIAL as material, D_BIC_ZDISTR_CH as bic_zdistr_ch, D_REGION as state, D_BIC_ZG_AVV004 as bic_zg_avv004, D_BIC_ZNT_WT_LB as bic_znt_wt_lb\n",
    "      from fact\n",
    "      where D_FISCYEAR IN ('2020','2019')) a\n",
    " left join hierarchies_customer b\n",
    "     on a.customer = b.store_id\n",
    "     and a.bic_zdistr_ch = b.distribution_channel_id_cust\n",
    " left join hierarchies_product c\n",
    "     on a.material = c.sku_id\n",
    "     and a.bic_zdistr_ch = c.distribution_channel_id_prod\n",
    ") g\n",
    "group by g.date0, g.fiscper, g.promoted_product_group, g.promoted_product_group_desc, g.product_family_desc, g.product_segment_name, g.product_category_name, g.management_grouping_desc, g.state, g.ac_scbm_id, g.ac_scbm_desc, g.bic_zdistr_ch, g.state\n",
    "\"\"\"\n",
    "combined_raw_sku = spark.sql(query)\n",
    "combined_raw_sku = combined_raw_sku.withColumn('date', next_day(to_date(col(\"date0\"), \"yyyyMMdd\"), \"saturday\"))\n",
    "\n",
    "#z.show(combined_raw_sku)\n",
    "\n",
    "# %pyspark\n",
    "# Aggregate daily data to weekly\n",
    "agg_raw = combined_raw_sku.groupBy(\"date\", \"promoted_product_group\", \"promoted_product_group_desc\", \n",
    "\t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\",\n",
    "\t\"state\", \"ac_scbm_id\", \"ac_scbm_desc\", \"bic_zdistr_ch\").\\\n",
    "agg({'gross_sales':'sum','sales_pounds':'sum'}).\\\n",
    "withColumnRenamed(\"sum(gross_sales)\",\"gross_sales\").\\\n",
    "withColumnRenamed(\"sum(sales_pounds)\",\"sales_pounds\")\n",
    "\n",
    "agg_raw = agg_raw.cache()\n",
    "\n",
    "print(agg_raw.count())\n",
    "z.show(agg_raw)\n",
    "\n",
    "# %pyspark\n",
    "# WIP - Write out Aggregated raw table for query / QC\n",
    "agg_raw.createOrReplaceTempView(\"raw_ship_agg\")\n",
    "spark.sql(\"drop table if exists default.raw_ship_agg\")\n",
    "spark.sql(\"create table if not exists default.raw_ship_agg as select * from raw_ship_agg\")\n",
    "\n",
    "# %pyspark\n",
    "# agg_raw_week = agg_raw_nodup_ppg_hier.withColumn(\"weekOfYear\", weekofyear(col(\"date\")))\n",
    "agg_raw_week = agg_raw.withColumn(\"weekOfYear\", weekofyear(col(\"date\")))\n",
    "agg_raw_week = agg_raw_week.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "agg_raw_week_curr = agg_raw_week.withColumn(\"year\", expr(\"year - 1\"))\n",
    "\n",
    "agg_raw_week_prev = agg_raw_week.select(\"year\", \"weekOfYear\", 'promoted_product_group', \"promoted_product_group_desc\", \n",
    "\t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\", \"state\", \"ac_scbm_id\", \"ac_scbm_desc\", \"bic_zdistr_ch\", \"sales_pounds\", \"gross_sales\").\\\n",
    "withColumnRenamed(\"sales_pounds\", \"stly_sales_pounds\").withColumnRenamed(\"gross_sales\", \"stly_gross_sales\")\n",
    "\n",
    "# Fill nulls with NAs to enable correct join in Spark\n",
    "agg_raw_week_curr = agg_raw_week_curr.fillna(\"Null_Value\", subset=['promoted_product_group',\"promoted_product_group_desc\", \n",
    "\t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\",'state','ac_scbm_id','ac_scbm_desc','bic_zdistr_ch'])\n",
    "\t\n",
    "agg_raw_week_prev = agg_raw_week_prev.fillna(\"Null_Value\", subset=['promoted_product_group',\"promoted_product_group_desc\", \n",
    "\t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\",'state','ac_scbm_id','ac_scbm_desc','bic_zdistr_ch'])\n",
    "\n",
    "# NOTE: Dataset has blank weekending dates but correct weekOfYear number. Use future dates (beyond curr week) to create a weekOfYear to weekending mapping file and retrieve the weekending.\n",
    "agg_week = agg_raw_week_curr.join(agg_raw_week_prev, on = ['year','weekOfYear','promoted_product_group',\"promoted_product_group_desc\", \n",
    "\t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\",'state','ac_scbm_id','ac_scbm_desc','bic_zdistr_ch'], how = 'outer')\n",
    "z.show(agg_week.filter(\"year >= 2019\"))\n",
    "\n",
    "# %pyspark\n",
    "week_to_we_map = agg_raw_week_curr.filter(\"year = 2019\").select(\"weekOfYear\",\"date\").distinct()\n",
    "z.show(week_to_we_map)\n",
    "\n",
    "# %pyspark\n",
    "# agg_week_2019_only = agg_raw_week_prev.join(agg_raw_week_curr, on = ['year','weekOfYear','promoted_product_group',\"promoted_product_group_desc\", \n",
    "# \t\"product_family_desc\", \"product_segment_name\", \"product_category_name\", \"management_grouping_desc\",'state','ac_scbm_id','ac_scbm_desc','bic_zdistr_ch'], how = 'left_anti')\n",
    "# z.show(agg_week_2019_only.filter(\"year >= 2019\"))\n",
    "# agg_week_2019_only.write.format('parquet').mode('overwrite').option('header','true').save('/user/bwn2456/CBDA/ship_agg_week_2019_only/')\n",
    "\n",
    "# %pyspark\n",
    "# Channel mapping file\n",
    "from pyspark.sql.types import *\n",
    "channel_mapping = spark.createDataFrame(\n",
    "[\n",
    "(\"01\", \"Distribution Channel 01\"),\n",
    "(\"10\", \"Other\"),\n",
    "(\"11\", \"DSD Bis Intercompany\"),\n",
    "(\"12\", \"DSD Pizza Intercomp\"),\n",
    "(\"20\", \"\"\"Warehouse/Exports\"\"\"),\n",
    "(\"30\", \"Foodservice\"),\n",
    "(\"40\", \"DSD Pizza\"),\n",
    "(\"45\", \"DSD\"),\n",
    "(\"50\", \"KFI\"),\n",
    "(\"55\", \"Plant Ingredient\"),\n",
    "(\"60\", \"Imports\"),\n",
    "(\"65\", \"Bulk FS - Specialty\"),\n",
    "],\n",
    "StructType([StructField('bic_zdistr_ch',StringType(), True), StructField('channel_desc',StringType(), True)]) # add your columns label here\n",
    ")\n",
    "z.show(channel_mapping)\n",
    "\n",
    "# %pyspark\n",
    "agg_raw_chn = agg_week.join(channel_mapping, how='left', on='bic_zdistr_ch')\n",
    "\n",
    "# %pyspark\n",
    "# Write out\n",
    "agg_out = agg_raw_chn.filter(\"year = 2019\").filter(\"management_grouping_desc IS NOT NULL\")\n",
    "\n",
    "agg_out.createOrReplaceTempView(\"agg_out\") \n",
    "spark.sql(\"DROP TABLE IF EXISTS default.cbda_ship\")\n",
    "spark.sql(\"create table if not exists default.cbda_ship as select * from agg_out\")\n",
    "\n",
    "# %pyspark\n",
    "agg_model_cols = ['week_ending_date', 'channel', 'retailer', 'state', 'mdlz_business', 'mdlz_category', 'mdlz_brand', 'mdlz_ppg', 'promoted_product_group_desc', 'product_segment_name', 'retailer_desc', 'pos_qty', 'pos_dollar', 'week_of_year', 'year']\n",
    "\n",
    "agg_model = agg_raw_chn.filter(\"management_grouping_desc IS NOT NULL\")\n",
    "agg_model = agg_model.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "agg_model = agg_model.withColumnRenamed(\"date\",\"week_ending_date\").withColumnRenamed(\"ac_scbm_id\",\"retailer\").withColumnRenamed(\"management_grouping_desc\",\"mdlz_business\").withColumnRenamed(\"product_category_name\",\"mdlz_category\").withColumnRenamed(\"product_family_desc\",\"mdlz_brand\").withColumnRenamed(\"promoted_product_group\",\"mdlz_ppg\").withColumnRenamed(\"sales_pounds\",\"pos_qty\").withColumnRenamed(\"gross_sales\",\"pos_dollar\").withColumnRenamed(\"weekOfYear\",\"week_of_year\").withColumnRenamed(\"bic_zdistr_ch\",\"channel\").withColumnRenamed(\"ac_scbm_desc\", \"retailer_desc\").select(*agg_model_cols)\n",
    "\n",
    "z.show(agg_model)\n",
    "\n",
    "# %pyspark\n",
    "agg_model_all_cols = agg_raw_chn.withColumnRenamed(\"date\",\"week_ending_date\").withColumnRenamed(\"ac_scbm_id\",\"retailer\").withColumnRenamed(\"management_grouping_desc\",\"mdlz_business\").withColumnRenamed(\"product_category_name\",\"mdlz_category\").withColumnRenamed(\"product_family_desc\",\"mdlz_brand\").withColumnRenamed(\"promoted_product_group\",\"mdlz_ppg\").withColumnRenamed(\"sales_pounds\",\"pos_qty\").withColumnRenamed(\"gross_sales\",\"pos_dollar\").withColumnRenamed(\"weekOfYear\",\"week_of_year\").withColumnRenamed(\"bic_zdistr_ch\",\"channel\")\n",
    "z.show(agg_model_all_cols.filter(\"retailer = 'US3000422' AND state = 'CA' AND channel = '20' AND mdlz_ppg = 'FS8'\"))\n",
    "\n",
    "# %pyspark\n",
    "print(agg_model.count())\n",
    "print(agg_model.select(\"retailer\",\"state\",\"mdlz_business\",\"mdlz_category\",\"mdlz_brand\",\"mdlz_ppg\",\"channel\").distinct().count())\n",
    "\n",
    "# %pyspark\n",
    "# Write out\n",
    "agg_model.createOrReplaceTempView(\"agg_model\")\n",
    "spark.sql(\"drop table if exists default.cbda_ship_model_input\")\n",
    "spark.sql(\"create table default.cbda_ship_model_input as select * from agg_model\")\n",
    "\n",
    "# %pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
