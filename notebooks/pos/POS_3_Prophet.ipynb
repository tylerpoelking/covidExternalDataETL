{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "from dateutil.relativedelta import relativedelta as rd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "max_train_date = '2020-02-15' # Do not change since weeks after this started displaying COVID effects\n",
    "\n",
    "curr_week_date = '2020-04-18' # Update to latest POS data week\n",
    "\n",
    "PATH = \"./pos_prophet_results/0430\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Raw POS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in POS data\n",
    "with pyodbc.connect(CONNECTION_STRING, autocommit=True) as conn:\n",
    "    pos_raw = pd.read_sql(\"SELECT * FROM default.cbda_pos_model_input\", conn)\n",
    "\n",
    "# Drop prefix created when reading the data from Hive table\n",
    "pos = pos_raw.copy()\n",
    "pos.columns = [i.split(\".\")[1] for i in pos.columns.values]\n",
    "    \n",
    "# Convert to datetime format\n",
    "pos['week_ending_date'] = pd.to_datetime(pos['week_ending_date'])\n",
    "\n",
    "# Create a unique identifier for PPG \n",
    "cols = ['mdlz_business', 'mdlz_category', 'mdlz_brand','mdlz_ppg']\n",
    "pos['ppg_id'] = pos[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# Create a unique identifier for PPG/State/Retailer \n",
    "cols = ['ppg_id', 'state', 'retailer']\n",
    "pos['sell_id'] = pos[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# BUSINESS FILTERS PROVIDED BY MDLZ \n",
    "pos_main = pos[pos['week_ending_date']>'2019-01-01'] # Week Ending Filter from January 2019\n",
    "pos_main = pos_main[~pos_main['mdlz_category'].isin(['None','Cookie','Display PRD'])] #Excluding low value categories \n",
    "pos_main = pos_main [~((pos_main['mdlz_ppg']=='') | (pos_main['mdlz_ppg'].isnull()))] # Excluding blank and null PPG values\n",
    "pos_main = pos_main[~((pos_main['mdlz_business']=='') & (pos_main['mdlz_category']=='') & (pos_main['mdlz_brand']=='') & (pos_main['mdlz_ppg']!=''))] # Excluding PPGs with blank product hierarchy\n",
    "pos_main = pos_main[(pos_main['pos_dollar']>0.0) & (pos_main['pos_qty']>0.0)] # Remove returns\n",
    "pos_main = pos_main[~(((pos_main['pos_dollar'].isna()) & (pos_main['pos_qty'].isna())))] # Remove null sales\n",
    "\n",
    "# Sort\n",
    "pos_main.sort_values(['sell_id','week_ending_date'], ascending=[True,True],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_main[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and filter for Common Products - with full history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products = pd.read_csv(f'./common_sell_id_pos_0418.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset for common sell_ids\n",
    "pos_main_model = pos_main.merge(common_products['sell_id'],on=['sell_id'],how='inner')\n",
    "\n",
    "# Training data - Pre-COVID period; Do not update this date\n",
    "pos_main_top = pos_main_model[pos_main_model['week_ending_date']<=max_train_date]\n",
    "\n",
    "# Sort values for prophet\n",
    "pos_main_top.sort_values(['sell_id','week_ending_date'], ascending=[True,True],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_groups = pos_main_top.groupby(['sell_id'])\n",
    "print(f'Modeling {len(product_groups.groups.keys())} unique sell_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_groups = pos_main_top.groupby(['sell_id'])\n",
    "\n",
    "# Initialize variables\n",
    "start_time = time.time()\n",
    "all_sell_id_list = []\n",
    "counter = 0\n",
    "big_counter = 0\n",
    "total_sell_id = len(product_groups.groups.keys()) \n",
    "\n",
    "print(f'Modeling {len(product_groups.groups.keys())} unique sell_ids')\n",
    "\n",
    "# Loop over for each time series and generate forecasts\n",
    "for key in product_groups.groups.keys():\n",
    "    train = product_groups.get_group(key).reset_index()[['week_ending_date','pos_qty']]\n",
    "    train.columns=['ds','y']\n",
    "    max_val = train[train.ds <= '2019-08-01'][\"y\"].max() # Since only 1 year of training data is used, apply logistic growth to prevent overfitting\n",
    "    min_val = train[train.ds <= '2019-08-01'][\"y\"].min() # Since only 1 year of training data is used, apply logistic growth to prevent overfitting\n",
    "    train[\"cap\"] = max_val\n",
    "    train[\"floor\"] = min_val\n",
    "    m = Prophet(growth='logistic', yearly_seasonality=20, changepoint_prior_scale=0.05, \n",
    "                weekly_seasonality = False, daily_seasonality = False, seasonality_prior_scale=1,\n",
    "               uncertainty_samples=10)\n",
    "    m.fit(train)\n",
    "    \n",
    "    future = m.make_future_dataframe(periods=25, freq='W',include_history=False)\n",
    "    future[\"cap\"] = max_val\n",
    "    future[\"floor\"] = min_val\n",
    "    fcst = m.predict(future)\n",
    "    fcst = fcst[['ds','yhat','trend']]\n",
    "    \n",
    "    sell_id_predict_df = pd.concat([train,fcst], axis=0, ignore_index=True)\n",
    "    sell_id_predict_df['sell_id'] = key\n",
    "    all_sell_id_list.append(sell_id_predict_df)\n",
    "    \n",
    "    counter += 1\n",
    "    big_counter += 1\n",
    "    \n",
    "    if counter==500 and big_counter<total_sell_id:\n",
    "        timestr = time.strftime(\"%H%M%S\")\n",
    "        filename = timestr + \".feather\"\n",
    "        full_path = os.path.join(PATH, filename)\n",
    "        \n",
    "        # Flush out the output file along with timestamp\n",
    "        output_df = pd.concat(all_sell_id_list, axis=0, ignore_index=True)\n",
    "        #output_df.to_feather(f'{full_path}')\n",
    "        \n",
    "        # Reset the counter & dataframe\n",
    "        counter = 0\n",
    "        all_sell_id_list = []\n",
    "    \n",
    "    elif big_counter == total_sell_id:\n",
    "        print (\"Final Sell ID\")\n",
    "        timestr = time.strftime(\"%H%M%S\")\n",
    "        filename = timestr + \".feather\"\n",
    "        full_path = os.path.join(PATH, filename)\n",
    "        \n",
    "        # Flush out the output file along with timestamp\n",
    "        output_df = pd.concat(all_sell_id_list, axis=0, ignore_index=True)\n",
    "        print(\"ERROR!! Not writing to feather due to existing feather files!\")\n",
    "        #output_df.to_feather(f'{full_path}')\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"--- Total Training+Predicting ---\",{np.round(end - start_time,2)},'s')\n",
    "#print(\"--- Time Per PPG: %s seconds ---\",{np.round(((end - start_time)/total_sell_id),2)},'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the model output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the smaller feather files and concatenate\n",
    "print(f\"Path: {PATH}/*.feather\")\n",
    "all_files = glob.glob(PATH + \"/*.feather\")\n",
    "# all_files = glob.glob(PATH + \"Prophet Model Output.feather\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_feather(filename)\n",
    "    li.append(df)\n",
    "\n",
    "final = pd.concat(li, axis=0, ignore_index=True).drop([\"cap\",\"floor\",\"trend\"],axis=1)\n",
    "final.rename(columns={'ds':'week_ending_date','y':'pos_qty','yhat':'forecast_quantity'}, inplace=True)\n",
    "final_raw = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Saturday\n",
    "def give_week_ending(date):\n",
    "    start = date - timedelta(days=date.weekday())\n",
    "    end = start + timedelta(days=5)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the week ending to fall on a saturday\n",
    "final['week_ending_date'] = final['week_ending_date'].apply(lambda x: give_week_ending(x))\n",
    "\n",
    "# Substitute last training week of actuals in the forecast value for the viz purpose \n",
    "final.loc[(final['week_ending_date']==max_train_date),'forecast_quantity'] = final['pos_qty']\n",
    "\n",
    "# Remove duplicate record for latest week\n",
    "final = final[~((final['week_ending_date']==max_train_date) & (final['forecast_quantity'].isnull()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2020 TY and 2019 LY columns\n",
    "final['week_of_year'] = final['week_ending_date'].dt.week\n",
    "final['year'] = final['week_ending_date'].dt.year\n",
    "\n",
    "final_2020 = final[final['year']==2020]\n",
    "final_2019 = final[final['year']==2019][['sell_id','week_of_year','pos_qty']]\n",
    "final_2020 = final_2020.drop('year', axis = 1)\n",
    "final_new = final_2020.merge(final_2019, on = ['sell_id','week_of_year'], how ='left')\n",
    "\n",
    "final_new = final_new.rename(columns={'pos_qty_y':'pos_qty_ly','pos_qty_x':'pos_qty_ty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if products are showing very high growth or decline\n",
    "final_diff = final_new.copy()\n",
    "\n",
    "final_diff[\"difference\"] = final_diff[\"forecast_quantity\"] - final_diff[\"pos_qty_ly\"]\n",
    "final_diff[\"percent_diff\"] = final_diff['difference']/final_diff['pos_qty_ly']\n",
    "final_diff = final_diff[final_diff.week_ending_date >= curr_week_date]\n",
    "final_diff_agg = final_diff.groupby([\"sell_id\"]).agg({\"percent_diff\":\"mean\"}).reset_index()\n",
    "\n",
    "# If high growth / decline then identify the product as such so as not to double count/apply the growth\n",
    "final_diff_agg['high_growth_decline'] = final_diff_agg['percent_diff'].abs() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out consolidated prophet model output feather file, to be read back in POS_2_Modeling.ipynb\n",
    "prht_out = final_new[final_new.week_ending_date > max_train_date].drop([\"pos_qty_ty\",\"week_of_year\",'pos_qty_ly'],axis=1).rename(columns={'forecast_quantity':'prht_frcst'}).reset_index(drop=True)\n",
    "prht_out = prht_out.merge(final_diff_agg[[\"sell_id\",\"high_growth_decline\"]], how=\"inner\", on=\"sell_id\")\n",
    "\n",
    "prht_out.to_feather(f'./pos_prophet_results/0430_cleaned/pos_0430_cleaned_v2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prht_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top sell_ids by pos_qty\n",
    "top_sell_ids = final.groupby('sell_id')['pos_qty'].sum().reset_index()\\\n",
    "                .sort_values('pos_qty',ascending=False)['sell_id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 sell_ids\n",
    "for sell_id in top_sell_ids:\n",
    "    to_chart =final[final['sell_id']==sell_id]\n",
    "    \n",
    "    print(sell_id)\n",
    "    fig = go.Figure()\n",
    "    # Create and style traces\n",
    "    fig.add_trace(go.Scatter(x=to_chart['week_ending_date'], y=to_chart['pos_qty'],name='pos_qty_ty',\n",
    "                             line=dict(color='blue', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=to_chart['week_ending_date'], y=to_chart['forecast_quantity'],name='forecast_quantity',\n",
    "                             line=dict(color='red', width=2)))\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title=sell_id)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
