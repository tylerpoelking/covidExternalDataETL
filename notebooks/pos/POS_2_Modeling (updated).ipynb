{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# ----------\n",
    "import pandas as pd\n",
    "import os\n",
    "#import os.path\n",
    "from os import path\n",
    "import glob\n",
    "import pyodbc\n",
    "import pysftp\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import requests, zipfile, io\n",
    "from datetime import datetime, timedelta\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BOX_PATH = '/Users/raksha/Box Sync/Mondelez: Demand forecasts during COVID-19/4. EDA & Descriptive analytics'\n",
    "LOCAL_PATH = '/Users/raksha/Box Sync/Accounts/Mondelez'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE with your Mondelez LAN ID & Password to allow the script to connect to Hive and download the raw POS dataset\n",
    "mdlz_lan_id = \"bwn2456\"\n",
    "mdlz_lan_pwd = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Hive to read in POS raw data\n",
    "CONNECTION_STRING = ';'.join(f\"\"\"\n",
    "Description=Hortonworks Knox DSN\n",
    "Driver=/opt/cloudera/hiveodbc/lib/universal/libclouderahiveodbc.dylib\n",
    "Host=mdzusvpclhdp101.mdzprod.local\n",
    "port=8443\n",
    "HttpPath=gateway/mondelez/hive\n",
    "schema=default\n",
    "ServiceDiscoveryMode=0\n",
    "HiveServerType=2\n",
    "AuthMech=3\n",
    "ThriftTransport=2\n",
    "SSL=1\n",
    "TwoWaySSL=0\n",
    "AllowSelfSignedServerCert=1\n",
    "uid={mdlz_lan_id}\n",
    "pwd={mdlz_lan_pwd}\n",
    "\"\"\".splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to edge node to write out POS model output data\n",
    "edge_node = pysftp.Connection(host=\"10.54.252.11\", username=mdlz_lan_id, password=mdlz_lan_pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [0. Raw data Connections](#first-bullet)\n",
    "* [1. Read in POS Data](#second-bullet)\n",
    "* [2. Read in State Projections](#third-bullet)\n",
    "* [3. Pre-Processing of Modeling Data](#fourth-bullet)\n",
    "* [4. Naive Modeling](#fifth-bullet)\n",
    "* [5. Milestone Modeling + Prophet Model Integration](#sixth-bullet)\n",
    "* [6. Final Output](#seventh-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Raw Data Connections <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read POS Input File\n",
    "with pyodbc.connect(CONNECTION_STRING, autocommit=True) as conn:\n",
    "    pos_raw = pd.read_sql(\"SELECT * FROM default.cbda_pos_model_input\", conn)\n",
    "\n",
    "# Read State Projections\n",
    "projections = pd.read_csv(f'{BOX_PATH}/Data/Projections/projections_state.csv')\n",
    "projections_dates = pd.read_csv(f'{BOX_PATH}/Data/Projections/peak_deaths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in POS Data <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "POS data is at Week/State/Retailer/PPG level. Data Available from 2018-12-08 to 2020-04-18 (69 weeks). For the purposes of our analysis we are \n",
    "fitlering out the following rows based on - \n",
    "\n",
    "Week_Ending_Date - Include data only from January 1st 2019\n",
    "MDLZ_Category - Exclude 'Cookie','None','Display PRD'\n",
    "MDLZ_PPG - Exclude PPGs which have no product hierarchy (Business, Category, Brand is blank), Exclude PPGs with blank and null values\n",
    "POS_Dollar - Exclude negative, 0 and Null values\n",
    "POS_Qty - Exclude negative, 0 and Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters for POS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop prefix created when reading the data from Hive table\n",
    "pos = pos_raw.copy()\n",
    "pos.columns = [i.split(\".\")[1] for i in pos.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos['week_ending_date'] = pd.to_datetime(pos['week_ending_date'])\n",
    "\n",
    "# Create a unique identifier for PPG \n",
    "cols = ['mdlz_business', 'mdlz_category', 'mdlz_brand','mdlz_ppg']\n",
    "pos['ppg_id'] = pos[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# Create a unique identifier for PPG/State/Retailer \n",
    "cols = ['ppg_id', 'state', 'retailer']\n",
    "pos['sell_id'] = pos[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "\n",
    "# BUSINESS FILTERS PROVIDED BY MDLZ \n",
    "pos_main = pos[pos['week_ending_date']>'2019-01-01'] # Week Ending Filter from January 2019\n",
    "pos_main = pos_main[~pos_main['mdlz_category'].isin(['None','Cookie','Display PRD'])] #Excluding low value categories \n",
    "pos_main = pos_main [~((pos_main['mdlz_ppg']=='') | (pos_main['mdlz_ppg'].isnull()))] # Excluding blank and null PPG values\n",
    "pos_main = pos_main[~((pos_main['mdlz_business']=='') & (pos_main['mdlz_category']=='') & (pos_main['mdlz_brand']=='') & (pos_main['mdlz_ppg']!=''))] # Excluding PPGs with blank product hierarchy\n",
    "pos_main = pos_main[(pos_main['pos_dollar']>0.0) & (pos_main['pos_qty']>0.0)] # Remove returns\n",
    "pos_main = pos_main[~(((pos_main['pos_dollar'].isna()) & (pos_main['pos_qty'].isna())))] # Remove null sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "print (\"POS Summary - Before the filters are applied\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(pos)))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(pos['pos_dollar'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(pos['pos_qty'])))\n",
    "print (\"Number of unique products\",pos['sell_id'].nunique())\n",
    "\n",
    "print (\"\\nPOS Summary - After the filters are applied\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(pos_main)))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(pos_main['pos_dollar'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(pos_main['pos_qty'])))\n",
    "print (\"Number of unique products:\",pos_main['sell_id'].nunique())\n",
    "\n",
    "print (\"\\n Revenue contribution (%) lost due to the filters\",round((1 - (np.nansum(pos_main['pos_dollar'])/np.nansum(pos['pos_dollar'])))*100,2))\n",
    "print (\"\\n Volume contribution (%) lost due to the filters\",round((1 - (np.nansum(pos_main['pos_qty'])/np.nansum(pos['pos_qty'])))*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the YoY Growth by State, Retailer, PPG (Consider only those weeks which have both 2020 and 2019 sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_main['week_of_year'] = pos_main['week_ending_date'].dt.week\n",
    "pos_main['year'] = pos_main['week_ending_date'].dt.year\n",
    "pos_main_2020 = pos_main[pos_main['year']==2020]\n",
    "pos_main_2019 = pos_main[pos_main['year']==2019]\n",
    "pos_main_2019 = pos_main_2019.drop('week_ending_date', axis = 1)\n",
    "pos_main_2019 = pos_main_2019.drop(['year','sell_id','ppg_id'], axis = 1)\n",
    "pos_main_2020 = pos_main_2020.drop('year', axis = 1)\n",
    "pos_main_new = pos_main_2020.merge(pos_main_2019, on = ['state', 'retailer', 'mdlz_business', 'mdlz_category', 'mdlz_brand', 'mdlz_ppg', 'week_of_year'], how ='left')\n",
    "pos_main_new = pos_main_new.rename(columns={'pos_qty_x':'pos_qty_ty', 'pos_dollar_x':'pos_dollar_ty', 'pos_qty_y':'pos_qty_ly', 'pos_dollar_y':'pos_dollar_ly'})\n",
    "pos_main_new['Growth_perc_sales'] = (pos_main_new['pos_dollar_ty'] - pos_main_new['pos_dollar_ly']) / pos_main_new['pos_dollar_ly']\n",
    "pos_main_new['Growth_perc_qty'] = (pos_main_new['pos_qty_ty'] - pos_main_new['pos_qty_ly']) / pos_main_new['pos_qty_ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_pos_week = pos_main_new['week_ending_date'].max()\n",
    "max_pos_week = pos_main_new['week_of_year'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of unique products after the YoY join\",pos_main_new['sell_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read in State Projections <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reading in the IHME state level projections to establish first, second, third milestones during the panic buying period, POS under social distancing period and New Normal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_week_ending(date):\n",
    "    start = date - timedelta(days=date.weekday())\n",
    "    end = start + timedelta(days=5)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_week_ending_incsun(peak_date):\n",
    "    dt = datetime.strptime(peak_date, '%Y-%m-%d')\n",
    "    \n",
    "    if dt.weekday()!=6:  \n",
    "        start = dt - timedelta(days=dt.weekday())\n",
    "        end = start + timedelta(days=5)\n",
    "    \n",
    "    if dt.weekday()==6:\n",
    "        start = dt - timedelta(days=dt.weekday())\n",
    "        end = start + timedelta(days=12)\n",
    "    \n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections_dates['peak_deaths_week'] = projections_dates['peak_deaths_date'].apply(lambda x: give_week_ending_incsun(x))\n",
    "projections_dates['thirtyperc_deaths_week'] = projections_dates['end_day'].apply(lambda x: give_week_ending_incsun(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-Processing of Modeling Data <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of Products based on it's 2019 and 2020 Lifespan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Filtering of Products based on Lifespan - Milestone modeling of data requires the products to have complete lifespan for 2019 and 2020 POS weeks. Hence the modeling data will be divided into datasets - \n",
    "1. Complete History - Milestone Modeling \n",
    "2. Insufficient History - Naive Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reattach the 2019 dataset \n",
    "pos_main_2019 = pos_main[pos_main['year']==2019]\n",
    "\n",
    "# Create a unique identifier for PPG \n",
    "cols = ['mdlz_business', 'mdlz_category', 'mdlz_brand','mdlz_ppg']\n",
    "pos_main_2019['ppg_id'] = pos_main_2019[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n",
    "\n",
    "# Create a unique identifier for PPG/State/Retailer \n",
    "cols = ['ppg_id', 'state', 'retailer']\n",
    "pos_main_2019['sell_id'] = pos_main_2019[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many product groups have 16 weeks in 2020? \n",
    "unique_product_group = pos_main_new.groupby(['sell_id']).agg({'week_ending_date': lambda x: x.nunique()}).reset_index()\n",
    "print (f\"losing number of products with lifespan less than {max_pos_week} weeks of data (%)\",round(1 - (len(unique_product_group[unique_product_group['week_ending_date']==max_pos_week]) / len(unique_product_group)),2)*100) # 76% of the product groups \n",
    "\n",
    "complete_2020 = unique_product_group[unique_product_group['week_ending_date']==max_pos_week] \n",
    "incomplete_2020 = unique_product_group[unique_product_group['week_ending_date']<max_pos_week] \n",
    "\n",
    "print (f\"Losing 2020 revenue contribution (%) due to dropping products with less than {max_pos_week} weeks of data\",round(1-sum(pos_main_new.merge(complete_2020,on=['sell_id'],how='inner')['pos_dollar_ty']) / sum(pos_main_new['pos_dollar_ty']),2)*100)\n",
    "print (f\"Losing 2020 volume contribution (%) due to dropping products with less than {max_pos_week} weeks of data\",round(1-sum(pos_main_new.merge(complete_2020,on=['sell_id'],how='inner')['pos_qty_ty']) / sum(pos_main_new['pos_qty_ty']),2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many product groups have 52 weeks in 2019? \n",
    "\n",
    "unique_product_group_2019 = pos_main_2019.groupby(['sell_id']).agg({'week_ending_date': lambda x: x.nunique()}).reset_index()\n",
    "print (\"Losing number of products with lifespan less than 52 weeks of data (%)\",round(1 - (len(unique_product_group_2019[unique_product_group_2019['week_ending_date']==52]) / len(unique_product_group_2019)),4)*100) \n",
    "\n",
    "complete_2019 = unique_product_group_2019[unique_product_group_2019['week_ending_date']==52] \n",
    "incomplete_2019 = unique_product_group_2019[unique_product_group_2019['week_ending_date']<52] \n",
    "\n",
    "print (\"Losing 2019 revenue contribution (%) due to dropping products with less than 52 weeks of data\",round(1-sum(pos_main_2019.merge(complete_2019,on=['sell_id'],how='inner')['pos_dollar']) / sum(pos_main_2019['pos_dollar']),4)*100)\n",
    "print (\"Losing 2019 volume contribution (%) due to dropping products with less than 52 weeks of data\",round(1-sum(pos_main_2019.merge(complete_2019,on=['sell_id'],how='inner')['pos_qty']) / sum(pos_main_2019['pos_qty']),4)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products that satisfy the lifespan filter for both 2019 and 2020 - \n",
    "print (\"2020 products\",len(complete_2020))\n",
    "print (\"2019 products\",len(complete_2019))\n",
    "print (\"Common products\",len(complete_2019.merge(complete_2020,how='inner',on=['sell_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_products = complete_2019.merge(complete_2020,how='inner',on=['sell_id'])\n",
    "print (f\"Losing 2020 & 2019 revenue contribution (%) due to dropping products with less than 52 weeks of 2019 data and {max_pos_week} weeks of 2020 data\",round(1-sum(pos_main.merge(common_products,on=['sell_id'],how='inner')['pos_dollar']) / sum(pos_main['pos_dollar']),4)*100)\n",
    "print (f\"Losing 2020 & 2019 volume contribution (%) due to dropping products with less than 52 weeks of 2019 data and {max_pos_week} weeks of 2020 data\",round(1-sum(pos_main.merge(common_products,on=['sell_id'],how='inner')['pos_qty']) / sum(pos_main['pos_qty']),4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Modeling data for products with insufficient history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = common_products.merge(incomplete_2020,how='outer',on=['sell_id']) #Adding common_products + incomplete_2020\n",
    "missing_products = list(set(pos_main_new['sell_id'].unique()) - set(one_df['sell_id'].unique())) # pos_new - one_df  = missing_products sell_id\n",
    "\n",
    "missing_products_data = pos_main_new[pos_main_new['sell_id'].isin(missing_products)] # Grab missing products data using sell_id\n",
    "incomplete_2020_data = pos_main_new[pos_main_new['sell_id'].isin(incomplete_2020['sell_id'].unique())] #less than <15 weeks products in 2020\n",
    "\n",
    "incomplete_data = incomplete_2020_data.append(missing_products_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Check - Number of unique products\")\n",
    "print (\"After all the business filters are applied and YoY calculation\",pos_main_new['sell_id'].nunique())\n",
    "print (\"Combination of modeling complete data, incomplete 2020 data, missing products\",len(common_products) + len(incomplete_2020) + len(missing_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Naive Model - Products with Insufficient History <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections['date'] = pd.to_datetime(projections['date'])\n",
    "projections['week_of_year'] = projections['date'].dt.week\n",
    "projections['year'] = projections['date'].dt.year\n",
    "projections['week_ending_date'] = projections['date'].apply(lambda x: give_week_ending(x))\n",
    "\n",
    "projections_weekly = projections.groupby(['State','week_ending_date','week_of_year','year']).agg({'deaths_mean':'sum','new_pop_affected':'sum'}).reset_index()\n",
    "projections_weekly.rename(columns={'State':'state'},inplace=True)\n",
    "\n",
    "date_list = projections_weekly[projections_weekly['year']==2020].groupby(['week_ending_date','week_of_year']).agg({'state':'count'}).reset_index().sort_values('week_ending_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_incomplete = pd.DataFrame()\n",
    "\n",
    "#product_groups = dataset.groupby(['mdlz_business','mdlz_category','mdlz_brand','mdlz_ppg','retailer','state','ppg_id','sell_id'])\n",
    "product_groups = incomplete_data.groupby(['mdlz_business','mdlz_category','mdlz_brand','mdlz_ppg','retailer','state','ppg_id','sell_id'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f'Modeling {len(product_groups.groups.keys())} unique sell_ids')\n",
    "for key in product_groups.groups.keys():\n",
    "    \n",
    "    data = product_groups.get_group(key).reset_index()\n",
    "    \n",
    "    data = data.merge(date_list[['week_ending_date','week_of_year']],on=['week_ending_date','week_of_year'],how='right').sort_values('week_ending_date')\n",
    "\n",
    "    data['retailer'] = data[\"retailer\"].fillna(key[4])\n",
    "    data['state'] = data[\"state\"].fillna(key[5])\n",
    "    data['mdlz_business'] = data[\"mdlz_business\"].fillna(key[0])\n",
    "    data['mdlz_category'] = data[\"mdlz_category\"].fillna(key[1])\n",
    "    data['mdlz_brand'] = data[\"mdlz_brand\"].fillna(key[2])\n",
    "    data['mdlz_ppg'] = data[\"mdlz_ppg\"].fillna(key[3])\n",
    "    data['ppg_id'] = data[\"ppg_id\"].fillna(key[6])\n",
    "    data['sell_id'] = data[\"sell_id\"].fillna(key[7])\n",
    "    \n",
    "    # Fill the pos ty with 0 for the actual weeks \n",
    "    data.loc[data['week_ending_date']<=latest_pos_week,'pos_dollar_ty'] = data.loc[data['week_ending_date']<=latest_pos_week]['pos_dollar_ty'].fillna(0)\n",
    "    data.loc[data['week_ending_date']<=latest_pos_week,'pos_qty_ty'] = data.loc[data['week_ending_date']<=latest_pos_week]['pos_qty_ty'].fillna(0)\n",
    "    \n",
    "    ros = data.loc[data['week_ending_date']<=latest_pos_week]['pos_qty_ty'].mean()\n",
    "    \n",
    "    data['pos_actuals_avg'] = ros\n",
    "    \n",
    "    pos_incomplete = pos_incomplete.append(data)\n",
    "\n",
    "end = time.time()\n",
    "print(\"--- %s minutes ---\",(end - start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_incomplete.drop(['pos_qty_ly','pos_dollar_ly','index'],axis=1,inplace=True)\n",
    "\n",
    "pos_incomplete_raw = pos_incomplete.copy()\n",
    "\n",
    "pos_incomplete = pos_incomplete.merge(pos_main_2019[['sell_id','week_of_year','pos_qty','pos_dollar']],on=['sell_id','week_of_year'],how='left',suffixes=('', '_y'))\n",
    "pos_incomplete.rename(columns={'pos_qty':'pos_qty_ly','pos_dollar':'pos_dollar_ly'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_forecast(week,avg,qty_ty,qty_ly,state):\n",
    "    \n",
    "    if week==latest_pos_week:\n",
    "        return qty_ty\n",
    "    \n",
    "    elif week>latest_pos_week and state!='': #short lifespan product forecast\n",
    "        return avg\n",
    "    \n",
    "    elif week>latest_pos_week and state=='': #Null state product forecast\n",
    "        return qty_ly\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_incomplete['forecast_quantity'] = pos_incomplete[['week_ending_date','pos_actuals_avg','pos_qty_ty',\n",
    "                                                      'pos_qty_ly','state']].apply(lambda x: apply_forecast(*x),\n",
    "                                                                                   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Milestone Modeling + Prophet Model - Products with Complete POS History <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Modeling Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_data_raw = pos_main_new.merge(common_products['sell_id'],on=['sell_id'],how='inner')\n",
    "pos_model_data = pos_model_data_raw.copy()\n",
    "pos_model_data.drop(['retailer','mdlz_business','mdlz_category','mdlz_brand','mdlz_ppg','ppg_id','pos_qty_ly','pos_dollar_ly'],axis=1,inplace=True)\n",
    "\n",
    "pos_model_data['state_null'] = np.where(pos_model_data.state.isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model_data['sell_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataset on just the peak covid growth windows (3/14 & 3/21)\n",
    "maximum_peak_data = pos_model_data[(pos_model_data['week_of_year']>=11) & (pos_model_data['week_of_year']<=12)]\n",
    "\n",
    "# For each sell id, Identify the growth peak corresponding to those two weeks\n",
    "sell_maxium_peak = maximum_peak_data.loc[maximum_peak_data.groupby('sell_id')['Growth_perc_qty'].idxmax()][['sell_id','Growth_perc_qty']]\n",
    "sell_maxium_peak.rename(columns={'Growth_perc_qty':'covid_max_growth'},inplace=True)\n",
    "\n",
    "pos_model_data = pos_model_data.merge(sell_maxium_peak,on=['sell_id'],how='left')\n",
    "pos_model_data['negative_covid_impact'] = pos_model_data['covid_max_growth'].apply(lambda x: 1 if x<0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating 6 week rolling average after sorting the 2019 dataframe\n",
    "pos_main_2019 = pos_main_2019.sort_values(['sell_id','week_of_year'])\n",
    "rolling_average_window = 6\n",
    "pos_main_2019['pos_qty_rolling_6_week_med'] = pos_main_2019.groupby(['sell_id'])['pos_qty'].rolling(window = rolling_average_window).median().reset_index(0,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nPOS Model data Summary\")\n",
    "print (\"\\nNumber of rows: {0:,.0f}\".format(len(pos_model_data)))\n",
    "print (\"\\nNumber of unique products: {0:,.0f}\".format(len(pos_model_data['sell_id'].unique())))\n",
    "print (\"Total Dollars: {0:,.0f}\".format(np.nansum(pos_model_data['pos_dollar_ty'])))\n",
    "print (\"Total Quantity: {0:,.0f}\".format(np.nansum(pos_model_data['pos_qty_ty'])))\n",
    "\n",
    "print (\"\\nRevenue contribution (%) lost due to the product filters\",round((1 - (np.nansum(pos_model_data['pos_dollar_ty'])/np.nansum(pos_main_2020['pos_dollar']))),3)*100)\n",
    "print (\"\\nVolume contribution (%) lost due to the product filters\",round((1 - (np.nansum(pos_model_data['pos_qty_ty'])/np.nansum(pos_main_2020['pos_qty']))),3)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable declarations for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of future weeks to be projected \n",
    "date_list = projections_weekly[(projections_weekly['year']==2020)&(projections_weekly['week_of_year']<=31)].groupby(['week_ending_date','week_of_year']).agg({'state':'count'}).reset_index().sort_values('week_ending_date')\n",
    "pos_model_data = pos_model_data.sort_values(['sell_id','week_ending_date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_decay_multiplier(growth):\n",
    "    \n",
    "    if growth<0:\n",
    "        return 0.8\n",
    "    \n",
    "    elif growth*100 >= 0 and growth*100 <= 50:\n",
    "        return 0.2\n",
    "    \n",
    "    elif growth*100 > 50 and growth*100 <= 100:\n",
    "        return 0.15\n",
    "    \n",
    "    elif growth*100 > 100 and growth*100 <= 500:\n",
    "        return 0.1\n",
    "    \n",
    "    elif growth*100 > 500 and growth*100 <= 1000:\n",
    "        return 0.08\n",
    "    \n",
    "    else:\n",
    "        return 0.05    \n",
    "    \n",
    "def growth_decay_factor_post_latest_week(milestone):\n",
    "    \n",
    "    if milestone==1:\n",
    "        return 0.9\n",
    "    \n",
    "    elif milestone==2:\n",
    "        return 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged = pd.DataFrame(columns=['week_ending_date','sell_id','state','pos_qty_ty','pos_dollar_ty',\n",
    "                                   'week_of_year_x','Growth_perc_sales','Growth_perc_qty','week_of_year_y',\n",
    "                                   'first_milestone_date','second_milestone_date','third_milestone_date'])\n",
    "\n",
    "#product_groups = dataset[dataset['state_null']==0].groupby(['sell_id','state'])\n",
    "product_groups = pos_model_data[pos_model_data['state_null']==0].groupby(['sell_id','state'])\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Modeling {len(product_groups.groups.keys())} unique sell_ids')\n",
    "\n",
    "for key in product_groups.groups.keys():\n",
    "\n",
    "    data = product_groups.get_group(key).reset_index() \n",
    "    \n",
    "    state_value = key[1]\n",
    "    state_null_value = data['state_null'].iat[0]\n",
    "    covid_impact_value = data['negative_covid_impact'].iat[0]\n",
    "\n",
    "    \n",
    "    # To get future weeks for each unique product group\n",
    "    data = data.merge(date_list[['week_ending_date','week_of_year']], on=['week_ending_date'],how ='outer') \n",
    "    \n",
    "    # Fill in sell_id and state values for the future forecasting weeks \n",
    "    data['sell_id'] = data[\"sell_id\"].fillna(key[0])\n",
    "    data['state'] = data[\"state\"].fillna(key[1])\n",
    "    data['state_null'] = data[\"state_null\"].fillna(state_null_value)\n",
    "    data['negative_covid_impact'] = data[\"negative_covid_impact\"].fillna(covid_impact_value)\n",
    "    \n",
    "    \n",
    "    ############################################ Growth Decay Modeling ######################################################\n",
    "    \n",
    "    growth_peak_dates = ['2020-03-14','2020-03-21'] \n",
    "    death_peak_date = projections_dates[projections_dates['Geography'] == 'United States of America']['peak_deaths_week'].iat[0]\n",
    "    death_thirty_perc_date = projections_dates[projections_dates['state'] == state_value]['thirtyperc_deaths_week'].iat[0]\n",
    "\n",
    "    \n",
    "    # Before covid outbreak median & average growth\n",
    "    median = data[(data['week_ending_date']>='2020-01-01') & (data['week_ending_date'] <'2020-03-01')]['Growth_perc_qty'].median() \n",
    "    average = data[(data['week_ending_date']>='2020-01-01') & (data['week_ending_date'] <'2020-03-01')]['Growth_perc_qty'].mean()\n",
    "    \n",
    "    # Find the absolute peak growth value and it's corresponding week \n",
    "    peak_growth_value = data.loc[data[(data['week_ending_date']>=growth_peak_dates[0]) & (data['week_ending_date']<=growth_peak_dates[1])]['Growth_perc_qty'].idxmax()]['Growth_perc_qty']\n",
    "    peak_growth_date = data.loc[data[(data['week_ending_date']>=growth_peak_dates[0]) & (data['week_ending_date']<=growth_peak_dates[1])]['Growth_perc_qty'].idxmax()]['week_ending_date']\n",
    "    \n",
    "    \n",
    "    data['forecast1'] = 0.0\n",
    "    data['forecast_quantity'] = 0.0\n",
    "\n",
    "    \n",
    "    # Replicate the peak growth value as the very first forecast value - just for visualization (including stop gap measure)\n",
    "    latest_growth_value = data.loc[data['week_ending_date'] == latest_pos_week]['Growth_perc_qty'].iat[0]\n",
    "    latest_pos_value = data.loc[data['week_ending_date'] == latest_pos_week]['pos_qty_ty'].iat[0]\n",
    "\n",
    "    \n",
    "    data.loc[(data['week_ending_date'] == latest_pos_week),'forecast1'] = latest_growth_value\n",
    "    data.loc[(data['week_ending_date'] == latest_pos_week),'forecast_quantity'] = latest_pos_value\n",
    "\n",
    "    ############################################  Milestone Modeling     ###################################\n",
    "    \n",
    "    #     POS under social distancing: latest pos week - second milestone date \n",
    "    #     ramp down: second milestone date - third milestone date =  2 weeks window\n",
    "    #     new normal: >third milestone date \n",
    "    \n",
    "    \n",
    "    first_milestone_date = death_peak_date #US specific\n",
    "    second_milestone_date = death_thirty_perc_date\n",
    "    \n",
    "      \n",
    "    # Shift the second milestone date only\n",
    "    if (second_milestone_date - latest_pos_week).days/7 >=6:\n",
    "        second_milestone_date = death_thirty_perc_date - timedelta(days=28)\n",
    "        \n",
    "    state_open_date = second_milestone_date + timedelta(days=14) \n",
    "    \n",
    "    \n",
    "    ############### Check Forecast Type  ###############\n",
    "    \n",
    "    if (second_milestone_date - latest_pos_week).days/7 <=2:\n",
    "        data['fc_type'] = 1\n",
    "        \n",
    "        # New Normal Forecast\n",
    "        data.loc[(data['week_ending_date']> latest_pos_week),'forecast1'] = median\n",
    "    \n",
    "    elif (second_milestone_date - latest_pos_week).days/7 ==3:\n",
    "        data['fc_type'] = 2\n",
    "        \n",
    "        # Ramp Down period\n",
    "        second_milestone_range = data.loc[(data['week_ending_date'] >= latest_pos_week) & (data['week_ending_date']< state_open_date),'forecast1'].index.values\n",
    "\n",
    "        \n",
    "        for i in range(second_milestone_range[0], second_milestone_range[-1]+1):\n",
    "            data.loc[i+1, 'forecast1'] = data.loc[i, 'forecast1'] * growth_decay_factor_post_latest_week(2)\n",
    "   \n",
    "        # New Normal period\n",
    "        data.loc[(data['week_ending_date']>= state_open_date),'forecast1'] = median\n",
    "    \n",
    "    \n",
    "    elif (second_milestone_date - latest_pos_week).days/7 > 3:\n",
    "        data['fc_type'] = 3\n",
    "    \n",
    "        # POS Under social distancing \n",
    "        first_milestone_range = data.loc[(data['week_ending_date'] >= latest_pos_week) & (data['week_ending_date'] < second_milestone_date),'forecast1'].index.values\n",
    "\n",
    "\n",
    "        for i in range(first_milestone_range[0], first_milestone_range[-1]+1):\n",
    "            data.loc[i+1, 'forecast1'] = data.loc[i, 'forecast1'] * growth_decay_factor_post_latest_week(1)\n",
    "\n",
    "            if data.loc[i,'negative_covid_impact']==0:\n",
    "                data.loc[i+1, 'forecast_quantity'] = data.loc[i, 'forecast_quantity'] * growth_decay_factor_post_latest_week(1)\n",
    "\n",
    "            else:\n",
    "                data.loc[i+1, 'forecast_quantity'] = data.loc[i, 'forecast_quantity'] / growth_decay_factor_post_latest_week(1)\n",
    "\n",
    "        # Ramp Down \n",
    "        second_milestone_range = data.loc[(data['week_ending_date'] >= second_milestone_date) & (data['week_ending_date']< state_open_date),'forecast1'].index.values\n",
    "\n",
    "        data.loc[second_milestone_range[0]:second_milestone_range[0]+1, 'forecast_quantity'] = 0   # Setting this as 0 in the first milestone range = 1 week only\n",
    "\n",
    "        for i in range(second_milestone_range[0], second_milestone_range[-1]+1):\n",
    "            data.loc[i+1, 'forecast1'] = data.loc[i, 'forecast1'] * growth_decay_factor_post_latest_week(2)\n",
    "\n",
    "        # New Normal \n",
    "        data.loc[(data['week_ending_date']>= state_open_date),'forecast1'] = median\n",
    "\n",
    "        \n",
    "    data['first_milestone_date'] = death_peak_date \n",
    "    data['second_milestone_date'] = second_milestone_date\n",
    "    data['third_milestone_date'] = state_open_date\n",
    "    data['median_baseline'] = median\n",
    "    \n",
    "    pos_merged = pos_merged.append(data)\n",
    "\n",
    "end = time.time()\n",
    "print(\"--- %s minutes ---\",(end - start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the model output df \n",
    "pos_merged_raw = pos_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing of the Milestone Model Output & Integration of Prophet Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged = pos_merged.drop(['week_of_year_x','index','covid_max_growth'],axis=1)\n",
    "pos_merged.rename(columns={'week_of_year_y':'week_of_year'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge model output with 2019 dataset to obtain the pos_qty_ly\n",
    "pos_merged = pos_merged.merge(pos_main_2019,on=['sell_id','week_of_year'],how='left',suffixes=('', '_y'))\n",
    "pos_merged.rename(columns={'pos_qty':'pos_qty_ly','pos_dollar':'pos_dollar_ly'},inplace=True)\n",
    "pos_merged.drop(['week_ending_date_y','state_y','year'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Prohpet's forecasts\n",
    "prht_fcst_common = pd.read_feather(f'{BOX_PATH}/Model Output/POS Prophet Model/pos_0430_cleaned_v2.feather')\n",
    "\n",
    "# Calculate rolling 6 week median of prophet's forecast to exclude effect of promotions\n",
    "prht_fcst_common = prht_fcst_common.sort_values(['sell_id','week_ending_date'])\n",
    "rolling_average_window = 6\n",
    "prht_fcst_common['pos_qty_prht_rolling_6_week_med'] = prht_fcst_common.groupby(['sell_id'])['prht_frcst'].rolling(window = rolling_average_window).median().reset_index(0,drop=True)\n",
    "\n",
    "prht_fcst_common = prht_fcst_common.drop(['prht_frcst'], axis=1)\n",
    "\n",
    "# prht_fcst_common.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in prophet's forecast for common products\n",
    "pos_merged = pos_merged.merge(prht_fcst_common, how='left', on=['sell_id','week_ending_date'])\n",
    "#pos_merged = pos_merged.merge(prht_fcst_high_grwth, how='left', on=['sell_id'])\n",
    "\n",
    "# Use prophet's forecast if available\n",
    "pos_merged = pos_merged.assign(pos_qty_rolling_6_week_med = \\\n",
    "                              np.where(~(pos_merged.pos_qty_prht_rolling_6_week_med.isnull()), \n",
    "                                       pos_merged.pos_qty_prht_rolling_6_week_med, \n",
    "                                       pos_merged.pos_qty_rolling_6_week_med))\n",
    "\n",
    "pos_merged = pos_merged.drop(['pos_qty_prht_rolling_6_week_med'],axis=1)\n",
    "\n",
    "# pos_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataset on just the peak covid growth windows (3/14 & 3/21)\n",
    "maximum_peak_data = pos_merged[(pos_merged['week_of_year']>=11) & (pos_merged['week_of_year']<=12)]\n",
    "\n",
    "# For each sell id, Identify the growth peak corresponding to those two weeks\n",
    "sell_maxium_ly = maximum_peak_data.loc[maximum_peak_data.groupby('sell_id')['Growth_perc_qty'].idxmax()][['sell_id','pos_qty_ly']]\n",
    "sell_maxium_ly.rename(columns={'pos_qty_ly':'max_pos_ly'},inplace=True)\n",
    "\n",
    "pos_merged = pos_merged.merge(sell_maxium_ly,on=['sell_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust forecast quantity for positive growing products by controlling for promotions\n",
    "def apply_forecast_value_positive(week,growth,qty_ly_rolling,qty_ty,qty_ly_max,z,third_m,second_m,fcst_qty,high_growth_decline):\n",
    "      \n",
    "    if week == latest_pos_week: # If the week is current pos week, set the forecast value to actual pos (just for viz purporse)\n",
    "        return qty_ty\n",
    "    \n",
    "    elif week>latest_pos_week and week<second_m:\n",
    "        return fcst_qty\n",
    "    \n",
    "    elif week>=second_m and high_growth_decline==True:\n",
    "        return qty_ly_rolling\n",
    "    \n",
    "    elif week>=second_m:\n",
    "        return (1 + growth) * qty_ly_rolling   \n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust forecast quantity for negative growing products by controlling for promotions\n",
    "def apply_forecast_value_negative(week,growth,qty_ly_rolling,qty_ty,qty_ly_max,z,third_m,second_m,fcst_qty,high_growth_decline):\n",
    "      \n",
    "    if week == latest_pos_week: # If the week is current pos week, set the forecast value to actual pos (just for viz purporse)\n",
    "        return qty_ty\n",
    "    \n",
    "    elif week>latest_pos_week and week<second_m:\n",
    "        return fcst_qty\n",
    "    \n",
    "    elif week>=second_m and week<third_m:\n",
    "        return (1 + growth) * qty_ly_max    \n",
    "    \n",
    "    elif week>=third_m and high_growth_decline==True:\n",
    "        return qty_ly_rolling\n",
    "    \n",
    "    elif week>=third_m:\n",
    "        return (1+growth) * qty_ly_rolling\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust forecast growth for positive growing products by controlling for outliers\n",
    "def floor_growth(median,growth,week,z): \n",
    "        \n",
    "    if week>latest_pos_week and growth<median:\n",
    "        return median\n",
    "    else:\n",
    "        return growth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust forecast growth for negative growing products by controlling for outliers\n",
    "def ceil_growth(median,growth,week,z): \n",
    "    \n",
    "    if week>latest_pos_week and growth>median:\n",
    "        return median\n",
    "    else:\n",
    "        return growth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged['forecast1'] = pos_merged[['median_baseline','forecast1','week_ending_date','negative_covid_impact']].\\\n",
    "                            apply(lambda x: floor_growth(*x) if x[3] == 0 else ceil_growth(*x),axis=1)\n",
    "\n",
    "pos_merged['forecast_quantity'] = pos_merged[['week_ending_date','forecast1','pos_qty_rolling_6_week_med',\n",
    "                                              'pos_qty_ty','max_pos_ly','negative_covid_impact',\n",
    "                                              'third_milestone_date','second_milestone_date','forecast_quantity',\n",
    "                                              'high_growth_decline']]\\\n",
    "                                    .apply(lambda x: apply_forecast_value_positive(*x) if x[5]==0  \\\n",
    "                                            else apply_forecast_value_negative(*x) ,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update final forecast quantity by controlling for outliers\n",
    "def update_forecast_positive(week,forecast1,fcst_growth,pos_qty_rolling,fcst_qty,first_m,third_m,neg_covid, high_growth_decline):\n",
    "\n",
    "    if week>latest_pos_week and week<third_m and high_growth_decline!=True: # Floor the value \n",
    "        if fcst_growth < forecast1:\n",
    "            return (1+forecast1) * pos_qty_rolling\n",
    "        else:\n",
    "            return fcst_qty\n",
    "    \n",
    "    else:\n",
    "        return fcst_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update final forecast quantity by controlling for outliers\n",
    "def update_forecast_negative(week,forecast1,fcst_growth,pos_qty_rolling,fcst_qty,first_m,third_m,neg_covid, high_growth_decline):\n",
    "\n",
    "    if week>latest_pos_week and week<third_m and high_growth_decline!=True: # Ceil the value\n",
    "        if fcst_growth > forecast1:\n",
    "            return (1+forecast1) * pos_qty_rolling\n",
    "        else:\n",
    "            return fcst_qty\n",
    "    \n",
    "    else:\n",
    "        return fcst_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_promo_uplift(week,second_m,qty_ly,qty_rolling):\n",
    "    \n",
    "    if week>=second_m:\n",
    "        return qty_ly - qty_rolling\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged['forecast_growth'] = (pos_merged['forecast_quantity'] - pos_merged['pos_qty_ly']) / (pos_merged['pos_qty_ly'])\n",
    "\n",
    "pos_merged['forecast_quantity_new'] = pos_merged[['week_ending_date','forecast1','forecast_growth',\n",
    "                                                  'pos_qty_rolling_6_week_med','forecast_quantity',\n",
    "                                                  'first_milestone_date','third_milestone_date',\n",
    "                                                  'negative_covid_impact','high_growth_decline']]\\\n",
    "                                            .apply(lambda x: update_forecast_positive(*x) if x[7]==0 else \n",
    "                                                   update_forecast_negative(*x) ,axis=1)\n",
    "\n",
    "pos_merged['promo_uplift'] = pos_merged[['week_ending_date','second_milestone_date','pos_qty_ly','pos_qty_rolling_6_week_med']].apply(lambda x: find_promo_uplift(*x),axis=1)\n",
    "\n",
    "pos_merged.drop(['forecast_quantity'],axis=1,inplace=True)\n",
    "pos_merged.rename(columns={'forecast_quantity_new':'forecast_quantity'},inplace=True)\n",
    "\n",
    "pos_merged.loc[pos_merged['week_ending_date']<latest_pos_week,'forecast1'] = np.nan\n",
    "pos_merged.loc[pos_merged['week_ending_date']<latest_pos_week,'forecast_quantity'] = np.nan\n",
    "pos_merged.loc[pos_merged['week_ending_date']<latest_pos_week,'forecast_growth'] = np.nan\n",
    "pos_merged.loc[pos_merged['week_ending_date']<latest_pos_week,'promo_uplift'] = np.nan\n",
    "\n",
    "# Save a copy for backup! 4/27 at 11:07 pm \n",
    "pos_merged_copy = pos_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged = pos_merged.sort_values(['sell_id','week_ending_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Without merging with the states data, number of rows\", len(pos_merged))\n",
    "print (\"Without merging with the states data, number of product groups\", int(len(pos_merged)/32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Output <a class=\"anchor\" id=\"seventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Naive, Milestone Model and Prophet Model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging modeling and short lifespan datasets\n",
    "pos_merged1 = pos_merged.append(pos_incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged1['sell_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged1['week_ending_date'] = pd.to_datetime(pos_merged1['week_ending_date'])\n",
    "pos_merged1['first_milestone_date'] = pd.to_datetime(pos_merged1['first_milestone_date'])\n",
    "pos_merged1['second_milestone_date'] = pd.to_datetime(pos_merged1['second_milestone_date'])\n",
    "pos_merged1['third_milestone_date'] = pd.to_datetime(pos_merged1['third_milestone_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"After merging with the states data, number of rows\", len(pos_merged1))\n",
    "print (\"After merging with the states data, number of product groups\", int(len(pos_merged1)/32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out Model output parquet file to edge node\n",
    "start = time.time()\n",
    "\n",
    "out_buffer = BytesIO()\n",
    "pos_merged1.to_parquet(out_buffer, index=False)\n",
    "\n",
    "destination_path = \"pos_model_results.parquet.gzip\"\n",
    "with edge_node.open(destination_path, 'w+', 32768) as f:\n",
    "    f.write(out_buffer.getvalue())\n",
    "    \n",
    "print('Writing out took', time.time()-start, 'seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
