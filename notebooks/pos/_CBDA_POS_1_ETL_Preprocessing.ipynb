{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "# Import libraries\n",
    "spark.version\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SQLContext\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window, Row\n",
    "\n",
    "# %pyspark\n",
    "# Read in raw POS weekly data and aggregate across channels since POS channels are not accurate and should not be used\n",
    "pos_no_channel = spark.sql(\n",
    "    \"select week_ending_date, retailer, state, mdlz_business, mdlz_category, mdlz_brand, mdlz_ppg, sum(pos_qty) as pos_qty, sum(pos_dollar) as pos_dollar \\\n",
    "from d4sa_us_disc.bluesky_pos_data \\\n",
    "group by week_ending_date, retailer, state, mdlz_business, mdlz_category, mdlz_brand, mdlz_ppg\"\n",
    ")\n",
    "\n",
    "pos_no_channel = pos_no_channel.withColumn(\n",
    "    \"week_of_year\", F.weekofyear(F.col(\"week_ending_date\"))\n",
    ")\n",
    "pos_no_channel = pos_no_channel.withColumn(\"year\", F.year(F.col(\"week_ending_date\")))\n",
    "pos_no_channel = pos_no_channel.withColumn(\"pos_qty\", F.col(\"pos_qty\").cast(\"double\"))\n",
    "pos_no_channel = pos_no_channel.withColumn(\n",
    "    \"pos_dollar\", F.col(\"pos_dollar\").cast(\"double\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    pos_no_channel.select(\n",
    "        \"retailer\", \"state\", \"mdlz_business\", \"mdlz_category\", \"mdlz_brand\", \"mdlz_ppg\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Write out for POS_2_Modeling script in Python\n",
    "pos_no_channel.createOrReplaceTempView(\"pos_no_channel\")\n",
    "spark.sql(\"drop table if exists default.cbda_pos_model_input\")\n",
    "spark.sql(\"create table default.cbda_pos_model_input as select * from pos_no_channel\")\n",
    "\n",
    "z.show(pos_no_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL FOR TABLEAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "# Read in projections data - Please upload the latest projections_state.csv file from Box (https://ibm.box.com/s/d4wu2qw32fzzm3xdgkyon7ynpiqo89p6)\n",
    "projections = spark.read.csv(\n",
    "    \"/user/bwn2456/CBDA/projections_state_0503.csv\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Read states name to state abbreviation mapping file. Example: \"Alaska\" -> \"AK\". This is a static file and does not need to be updated.\n",
    "states = spark.read.csv(\n",
    "    \"/user/bwn2456/CBDA/states.csv\", header=True, inferSchema=True\n",
    ").withColumnRenamed(\"state\", \"Geography\")\n",
    "\n",
    "# %pyspark\n",
    "# Business Filters\n",
    "print(\n",
    "    pos_no_channel.select(\n",
    "        \"retailer\", \"state\", \"mdlz_business\", \"mdlz_category\", \"mdlz_brand\", \"mdlz_ppg\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "pos_no_channel = pos_no_channel.filter(col(\"week_ending_date\") > lit(\"2019-01-01\"))\n",
    "pos_no_channel = pos_no_channel.filter(\n",
    "    \"mdlz_category NOT IN ('None','Cookie','Display PRD')\"\n",
    ")\n",
    "pos_no_channel = pos_no_channel.filter(\"NOT(mdlz_ppg == '' OR mdlz_ppg IS NULL)\")\n",
    "pos_no_channel = pos_no_channel.filter(\n",
    "    \"NOT(mdlz_business == '' AND mdlz_category == '' AND mdlz_brand == '' AND mdlz_ppg != '')\"\n",
    ")\n",
    "pos_no_channel = pos_no_channel.filter(\"pos_dollar > 0 AND pos_qty > 0\")\n",
    "pos_no_channel = pos_no_channel.filter(\"NOT(pos_dollar IS NULL AND pos_qty IS NULL)\")\n",
    "\n",
    "print(\n",
    "    pos_no_channel.select(\n",
    "        \"retailer\", \"state\", \"mdlz_business\", \"mdlz_category\", \"mdlz_brand\", \"mdlz_ppg\"\n",
    "    )\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# %pyspark\n",
    "# Create POS 2020 (This Year) and POS 2019 (Last Year) columns into a single dataframe for visualization in Tableau\n",
    "pos_2020 = pos_no_channel.filter(pos_no_channel[\"year\"] == 2020)\n",
    "pos_2019 = pos_no_channel.filter(pos_no_channel[\"year\"] == 2019)\n",
    "\n",
    "pos_2019 = pos_2019.drop(\"week_ending_date\")\n",
    "pos_2019 = pos_2019.drop(\"year\")\n",
    "pos_2020 = pos_2020.drop(\"year\")\n",
    "\n",
    "pos_2019 = pos_2019.withColumnRenamed(\"pos_dollar\", \"pos_dollar_ly\")\n",
    "pos_2019 = pos_2019.withColumnRenamed(\"pos_qty\", \"pos_qty_ly\")\n",
    "pos_2020 = pos_2020.withColumnRenamed(\"pos_dollar\", \"pos_dollar_ty\")\n",
    "pos_2020 = pos_2020.withColumnRenamed(\"pos_qty\", \"pos_qty_ty\")\n",
    "\n",
    "pos_updated = pos_2020.join(\n",
    "    pos_2019,\n",
    "    on=[\n",
    "        \"retailer\",\n",
    "        \"state\",\n",
    "        \"mdlz_business\",\n",
    "        \"mdlz_category\",\n",
    "        \"mdlz_brand\",\n",
    "        \"mdlz_ppg\",\n",
    "        \"week_of_year\",\n",
    "    ],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Transform projections to create state milestone data markers in Tableau\n",
    "projections = projections.select(\n",
    "    \"Geography\", \"end_day\", \"peak_deaths_date\", \"peak_affected_date\"\n",
    ")\n",
    "projections = projections.dropDuplicates()\n",
    "\n",
    "projections = projections.withColumn(\n",
    "    \"end_day\", F.split(projections[\"end_day\"], \" \").getItem(0)\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"peak_deaths_date\", F.split(projections[\"peak_deaths_date\"], \" \").getItem(0)\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"peak_affected_date\", F.split(projections[\"peak_affected_date\"], \" \").getItem(0)\n",
    ")\n",
    "\n",
    "projections = projections.withColumn(\"end_day\", projections[\"end_day\"].cast(DateType()))\n",
    "projections = projections.withColumn(\n",
    "    \"peak_deaths_date\", projections[\"peak_deaths_date\"].cast(DateType())\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"peak_affected_date\", projections[\"peak_affected_date\"].cast(DateType())\n",
    ")\n",
    "\n",
    "projections = projections.withColumn(\"end_day_dayofweek\", F.dayofweek(F.col(\"end_day\")))\n",
    "projections = projections.withColumn(\n",
    "    \"peak_deaths_date_dayofweek\", F.dayofweek(F.col(\"peak_deaths_date\"))\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"peak_affected_date_dayofweek\", F.dayofweek(F.col(\"peak_affected_date\"))\n",
    ")\n",
    "\n",
    "# Convert to Saturday\n",
    "def get_week_end_date(val):\n",
    "    return 7 - val\n",
    "\n",
    "\n",
    "udf_end_date = F.udf(get_week_end_date, IntegerType())\n",
    "\n",
    "projections = projections.withColumn(\"30_perc_date\", udf_end_date(\"end_day_dayofweek\"))\n",
    "projections = projections.withColumn(\n",
    "    \"peak_death_date\", udf_end_date(\"peak_deaths_date_dayofweek\")\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"peak_affect_date\", udf_end_date(\"peak_affected_date_dayofweek\")\n",
    ")\n",
    "\n",
    "projections = projections.withColumn(\n",
    "    \"week_ending_30\", F.expr(\"date_add(end_day, 30_perc_date)\")\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"week_ending_pd\", F.expr(\"date_add(peak_deaths_date, peak_death_date)\")\n",
    ")\n",
    "projections = projections.withColumn(\n",
    "    \"week_ending_pa\", F.expr(\"date_add(peak_affected_date, peak_affect_date)\")\n",
    ")\n",
    "\n",
    "projections = projections.select(\n",
    "    \"Geography\", \"week_ending_30\", \"week_ending_pd\", \"week_ending_pa\"\n",
    ")\n",
    "\n",
    "\n",
    "# Merge in state abbreviations\n",
    "projections_updated = projections.join(states, on=[\"Geography\"], how=\"inner\")\n",
    "# projections_updated = projections.join(states, (projections['Geography'] == states['Geography']), how='inner')\n",
    "\n",
    "# projections_updated = projections_updated.withColumnRenamed('state',\"state_raw\")\n",
    "# projections_updated = projections_updated.drop('state')\n",
    "projections_updated = projections_updated.drop(\"Geography\")\n",
    "\n",
    "projections_updated = projections_updated.withColumnRenamed(\"abbr\", \"state\")\n",
    "projections_updated = projections_updated.withColumn(\"ph\", F.lit(2.0))\n",
    "\n",
    "proj_30_pec = projections_updated.select(\"week_ending_30\", \"state\", \"ph\")\n",
    "proj_pd = projections_updated.select(\"week_ending_pd\", \"state\", \"ph\")\n",
    "proj_pa = projections_updated.select(\"week_ending_pa\", \"state\", \"ph\")\n",
    "\n",
    "proj_pd_pos = proj_pd.withColumnRenamed(\n",
    "    \"week_ending_pd\", \"week_ending_date\"\n",
    ").withColumnRenamed(\"ph\", \"week_ending_pd\")\n",
    "proj_pa_pos = proj_pa.withColumnRenamed(\n",
    "    \"week_ending_pa\", \"week_ending_date\"\n",
    ").withColumnRenamed(\"ph\", \"week_ending_pa\")\n",
    "proj_30_pec_pos = proj_30_pec.withColumnRenamed(\n",
    "    \"week_ending_30\", \"week_ending_date\"\n",
    ").withColumnRenamed(\"ph\", \"week_ending_30\")\n",
    "\n",
    "\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"week_ending_pa\", F.lit(None).cast(DoubleType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"week_ending_pd\", F.lit(None).cast(DoubleType())\n",
    ")\n",
    "\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"week_ending_30\", F.lit(None).cast(DoubleType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"week_ending_pd\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"week_ending_30\", F.lit(None).cast(DoubleType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"week_ending_pa\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "\n",
    "pos_updated = pos_updated.withColumn(\"week_ending_30\", F.lit(None).cast(DoubleType()))\n",
    "pos_updated = pos_updated.withColumn(\"week_ending_pd\", F.lit(None).cast(DoubleType()))\n",
    "pos_updated = pos_updated.withColumn(\"week_ending_pa\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\"retailer\", F.lit(None).cast(StringType()))\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"mdlz_business\", F.lit(None).cast(StringType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"mdlz_category\", F.lit(None).cast(StringType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"mdlz_brand\", F.lit(None).cast(StringType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\"mdlz_ppg\", F.lit(None).cast(StringType()))\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\"pos_qty_ty\", F.lit(None).cast(LongType()))\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"pos_dollar_ty\", F.lit(None).cast(DoubleType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"week_of_year\", F.lit(None).cast(IntegerType())\n",
    ")\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\"pos_qty_ly\", F.lit(None).cast(LongType()))\n",
    "proj_30_pec_pos = proj_30_pec_pos.withColumn(\n",
    "    \"pos_dollar_ly\", F.lit(None).cast(DoubleType())\n",
    ")\n",
    "\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"retailer\", F.lit(None).cast(StringType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"mdlz_business\", F.lit(None).cast(StringType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"mdlz_category\", F.lit(None).cast(StringType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"mdlz_brand\", F.lit(None).cast(StringType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"mdlz_ppg\", F.lit(None).cast(StringType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"pos_qty_ty\", F.lit(None).cast(LongType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"pos_dollar_ty\", F.lit(None).cast(DoubleType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"week_of_year\", F.lit(None).cast(IntegerType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"pos_qty_ly\", F.lit(None).cast(LongType()))\n",
    "proj_pa_pos = proj_pa_pos.withColumn(\"pos_dollar_ly\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"retailer\", F.lit(None).cast(StringType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"mdlz_business\", F.lit(None).cast(StringType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"mdlz_category\", F.lit(None).cast(StringType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"mdlz_brand\", F.lit(None).cast(StringType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"mdlz_ppg\", F.lit(None).cast(StringType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"pos_qty_ty\", F.lit(None).cast(LongType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"pos_dollar_ty\", F.lit(None).cast(DoubleType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"week_of_year\", F.lit(None).cast(IntegerType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"pos_qty_ly\", F.lit(None).cast(LongType()))\n",
    "proj_pd_pos = proj_pd_pos.withColumn(\"pos_dollar_ly\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "column_order = [\n",
    "    \"retailer\",\n",
    "    \"state\",\n",
    "    \"mdlz_business\",\n",
    "    \"mdlz_category\",\n",
    "    \"mdlz_brand\",\n",
    "    \"mdlz_ppg\",\n",
    "    \"week_of_year\",\n",
    "    \"week_ending_date\",\n",
    "    \"pos_qty_ty\",\n",
    "    \"pos_dollar_ty\",\n",
    "    \"pos_qty_ly\",\n",
    "    \"pos_dollar_ly\",\n",
    "    \"week_ending_30\",\n",
    "    \"week_ending_pd\",\n",
    "    \"week_ending_pa\",\n",
    "]\n",
    "\n",
    "proj_30_pec_pos = proj_30_pec_pos.select(\n",
    "    \"retailer\",\n",
    "    \"state\",\n",
    "    \"mdlz_business\",\n",
    "    \"mdlz_category\",\n",
    "    \"mdlz_brand\",\n",
    "    \"mdlz_ppg\",\n",
    "    \"week_of_year\",\n",
    "    \"week_ending_date\",\n",
    "    \"pos_qty_ty\",\n",
    "    \"pos_dollar_ty\",\n",
    "    \"pos_qty_ly\",\n",
    "    \"pos_dollar_ly\",\n",
    "    \"week_ending_30\",\n",
    "    \"week_ending_pd\",\n",
    "    \"week_ending_pa\",\n",
    ")\n",
    "proj_pa_pos = proj_pa_pos.select(\n",
    "    \"retailer\",\n",
    "    \"state\",\n",
    "    \"mdlz_business\",\n",
    "    \"mdlz_category\",\n",
    "    \"mdlz_brand\",\n",
    "    \"mdlz_ppg\",\n",
    "    \"week_of_year\",\n",
    "    \"week_ending_date\",\n",
    "    \"pos_qty_ty\",\n",
    "    \"pos_dollar_ty\",\n",
    "    \"pos_qty_ly\",\n",
    "    \"pos_dollar_ly\",\n",
    "    \"week_ending_30\",\n",
    "    \"week_ending_pd\",\n",
    "    \"week_ending_pa\",\n",
    ")\n",
    "proj_pd_pos = proj_pd_pos.select(\n",
    "    \"retailer\",\n",
    "    \"state\",\n",
    "    \"mdlz_business\",\n",
    "    \"mdlz_category\",\n",
    "    \"mdlz_brand\",\n",
    "    \"mdlz_ppg\",\n",
    "    \"week_of_year\",\n",
    "    \"week_ending_date\",\n",
    "    \"pos_qty_ty\",\n",
    "    \"pos_dollar_ty\",\n",
    "    \"pos_qty_ly\",\n",
    "    \"pos_dollar_ly\",\n",
    "    \"week_ending_30\",\n",
    "    \"week_ending_pd\",\n",
    "    \"week_ending_pa\",\n",
    ")\n",
    "pos_updated = pos_updated.select(\n",
    "    \"retailer\",\n",
    "    \"state\",\n",
    "    \"mdlz_business\",\n",
    "    \"mdlz_category\",\n",
    "    \"mdlz_brand\",\n",
    "    \"mdlz_ppg\",\n",
    "    \"week_of_year\",\n",
    "    \"week_ending_date\",\n",
    "    \"pos_qty_ty\",\n",
    "    \"pos_dollar_ty\",\n",
    "    \"pos_qty_ly\",\n",
    "    \"pos_dollar_ly\",\n",
    "    \"week_ending_30\",\n",
    "    \"week_ending_pd\",\n",
    "    \"week_ending_pa\",\n",
    ")\n",
    "\n",
    "# Append\n",
    "df_fin = pos_updated.union(proj_30_pec_pos)\n",
    "df_fin = df_fin.union(proj_pa_pos)\n",
    "df_fin = df_fin.union(proj_pd_pos)\n",
    "\n",
    "\n",
    "# %pyspark\n",
    "# Write out the data for post-model processing and integration into Tableau\n",
    "df_fin.createOrReplaceTempView(\"pos\")\n",
    "spark.sql(\"drop table if exists default.cbda_pos\")\n",
    "spark.sql(\"create table default.cbda_pos as select * from pos\")\n",
    "\n",
    "# %pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
